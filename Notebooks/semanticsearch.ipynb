{"cells":[{"cell_type":"markdown","metadata":{},"source":["## Install"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["pip install tensorflow"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["pip install python-dotenv"]},{"cell_type":"markdown","metadata":{},"source":["## Import Necessary Libraries"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2DwTz3CTuxfc"},"outputs":[],"source":["import pandas as pd\n","import tensorflow_hub as hub\n","import numpy as np\n","import tensorflow as tf\n","from sklearn.metrics.pairwise import cosine_similarity\n","from dotenv import load_dotenv"]},{"cell_type":"markdown","metadata":{},"source":["## Load Data"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":27948,"status":"ok","timestamp":1717396779935,"user":{"displayName":"Akbar Maulana Ibrahim M004D4KY3332","userId":"11704943035037494098"},"user_tz":-420},"id":"OW6SJaIT0rzn","outputId":"e817fa52-6cd0-4198-f854-346985d0b875"},"outputs":[{"name":"stdout","output_type":"stream","text":["Mounted at /content/drive\n"]}],"source":["load_dotenv()  # Load environment variables from .env file\n","jwt_token = os.getenv('JWT_TOKEN')\n","\n","headers = {'Authorization': f'Bearer {jwt_token}'}\n","\n","api_urls = {\n","    'interactions': 'http://161.97.109.65:3000/api/interactions/download',\n","    'users': 'http://161.97.109.65:3000/api/users/download',\n","    'products': 'http://161.97.109.65:3000/api/products/download'\n","}\n","\n","def fetch_data(url, headers):\n","    try:\n","        response = requests.get(url, headers=headers)\n","        response.raise_for_status()  # Raises an HTTPError for bad requests\n","        return pd.DataFrame(response.json())\n","    except requests.RequestException as e:\n","        print(f'Failed to fetch data from {url}: {str(e)}')\n","        return pd.DataFrame()\n","\n","# Fetch data from APIs\n","users = fetch_data(api_urls['users'], headers)\n","products = fetch_data(api_urls['products'], headers)\n","interactions = fetch_data(api_urls['interactions'], headers)\n","\n","if not users.empty and not products.empty and not interactions.empty:\n","    print(\"Data fetched successfully.\")\n","else:\n","    print(\"Data fetching failed, check errors and retry.\")\n","    # Optionally, add logic to halt further processing if data is crucial"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Ic9Dzmdxx0vD"},"outputs":[],"source":["titles = data['Title'].tolist()\n","labels = data['Label'].tolist()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"n4XGlJsVyP8z"},"outputs":[],"source":["# Combine title and label for better semantic understanding\n","combined_text = [f\"{label} {title}\" for label, title in zip(labels, titles)]\n","\n","# Load the Universal Sentence Encoder\n","embed = hub.load(\"https://tfhub.dev/google/universal-sentence-encoder/4\")\n","\n","# Generate embeddings for the product descriptions\n","embeddings = embed(combined_text)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7zXo0P8kyT3w"},"outputs":[],"source":["# Define the search function\n","def semantic_search(query, embeddings, data, top_k):\n","    # Generate the embedding for the query\n","    query_embedding = embed([query])\n","\n","    # Calculate cosine similarities\n","    similarities = cosine_similarity(query_embedding, embeddings).flatten()\n","    # Get the top_k products\n","    top_k_indices = np.where(similarities > 0.3)[0][-top_k:][::-1]\n","    if len(top_k_indices) == 0:\n","        return None  # Return None if no results found\n","    results = data.iloc[top_k_indices]\n","    return results"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":354,"status":"ok","timestamp":1717399763540,"user":{"displayName":"Akbar Maulana Ibrahim M004D4KY3332","userId":"11704943035037494098"},"user_tz":-420},"id":"xlx27-JwyYP3","outputId":"66511078-add1-4990-e25e-40246b893fb2"},"outputs":[{"name":"stdout","output_type":"stream","text":["                                                 Title    Harga  \\\n","423  Kasur Lantai MOLLORCA 100cm/kasurlipat/kasurva...   226.32   \n","409  IGOYO Kasur Lipat Navy 90/120/140/160x200cm Ma...    313.9   \n","394          Kasur Lipat Kasur Gulung Quantum Termurah  199.999   \n","391       Kasur lipat matras kasur lantai (90x170x5cm)   61.281   \n","384              Kasur Lantai Palembang Empuk Termurah       64   \n","377                             Kasur Busa Lipat Murah      130   \n","370         Kasur Lantai Lipat Palembang Varian Ukuran       85   \n","363       KASUR LIPAT BERKUALITAS MOTIF COWOK 90 x 170       65   \n","357  Kasur Lipat Lantai Bulu Rasfur Karakter 2 bant...      110   \n","345                                       kasur dewasa      185   \n","340  kasur palembang, kasur kapuk, kasur lantai, ka...       90   \n","324               KASUR LIPAT 90x170x5cm...SUPER MURAH     55.9   \n","295      Kasur Springbed Bigland Bigpoint [JAWA TIMUR]      498   \n","284  PROMO !!! KASUR LIPAT SINGLE TEBAL 10CM UK.70X...  104.279   \n","\n","            Asal Kota  Title_encoded  Label_encoded  \n","423       Kab. Bekasi            330              0  \n","409       Kab. Bekasi            224              0  \n","394     Jakarta Barat            334              0  \n","391     Kab. Sidoarjo            351              0  \n","384  Kab. Purbalingga            332              0  \n","377        Balikpapan            312              0  \n","370            Bekasi            329              0  \n","363     Kab. Sidoarjo            254              0  \n","357    Kab. Mojokerto            335              0  \n","345    Kab. Tangerang           1232              0  \n","340          Surabaya           1235              0  \n","324          Surabaya            253              0  \n","295          Surabaya            343              0  \n","284       Kab. Bantul            850              0  \n"]}],"source":["# Example usage\n","query = \"kasur lipat\"\n","results = semantic_search(query, embeddings, datas, top_k=20)\n","\n","print(results)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7RNK9izj8PDI"},"outputs":[],"source":["query_embedding = embed([query])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"HRyeZFZ08Iht"},"outputs":[],"source":["similarities = cosine_similarity(query_embedding, embeddings).flatten()\n","similarities = similarities[similarities>0.3]"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":725,"status":"ok","timestamp":1717079999947,"user":{"displayName":"Akbar Maulana Ibrahim M004D4KY3332","userId":"11704943035037494098"},"user_tz":-420},"id":"sXiyNRt_8Wwy","outputId":"cf577ecc-afc9-4cf4-ec5d-e84272914df4"},"outputs":[{"data":{"text/plain":["array([0.35749298, 0.3085053 , 0.3738201 , 0.33809584, 0.35383618,\n","       0.4525984 , 0.31888515, 0.34247783, 0.4525984 , 0.3547277 ,\n","       0.3175251 , 0.3517301 , 0.32293564], dtype=float32)"]},"execution_count":105,"metadata":{},"output_type":"execute_result"}],"source":["similarities"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1136,"status":"ok","timestamp":1717080047936,"user":{"displayName":"Akbar Maulana Ibrahim M004D4KY3332","userId":"11704943035037494098"},"user_tz":-420},"id":"rdJsMJ8n8fLg","outputId":"3c00003f-2d44-40c0-850c-1b866988ebc4"},"outputs":[{"data":{"text/plain":["array([8, 5, 2, 0, 9])"]},"execution_count":107,"metadata":{},"output_type":"execute_result"}],"source":["top_k_indices = similarities.argsort()[-5:][::-1]\n","top_k_indices"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"elapsed":107584,"status":"ok","timestamp":1717080450633,"user":{"displayName":"Akbar Maulana Ibrahim M004D4KY3332","userId":"11704943035037494098"},"user_tz":-420},"id":"qc1Qzlbg9oQC","outputId":"7b3718d9-e680-429d-91db-d5b7cf3ae457"},"outputs":[{"name":"stdout","output_type":"stream","text":["Collecting tensorflow_text\n","  Downloading tensorflow_text-2.16.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.2 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.2/5.2 MB\u001b[0m \u001b[31m29.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting tensorflow<2.17,>=2.16.1 (from tensorflow_text)\n","  Downloading tensorflow-2.16.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (589.8 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m589.8/589.8 MB\u001b[0m \u001b[31m1.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.17,>=2.16.1->tensorflow_text) (1.4.0)\n","Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.17,>=2.16.1->tensorflow_text) (1.6.3)\n","Requirement already satisfied: flatbuffers>=23.5.26 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.17,>=2.16.1->tensorflow_text) (24.3.25)\n","Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.17,>=2.16.1->tensorflow_text) (0.5.4)\n","Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.17,>=2.16.1->tensorflow_text) (0.2.0)\n","Collecting h5py>=3.10.0 (from tensorflow<2.17,>=2.16.1->tensorflow_text)\n","  Downloading h5py-3.11.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.3 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.3/5.3 MB\u001b[0m \u001b[31m48.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.17,>=2.16.1->tensorflow_text) (18.1.1)\n","Collecting ml-dtypes~=0.3.1 (from tensorflow<2.17,>=2.16.1->tensorflow_text)\n","  Downloading ml_dtypes-0.3.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.2 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.2/2.2 MB\u001b[0m \u001b[31m39.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.17,>=2.16.1->tensorflow_text) (3.3.0)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.17,>=2.16.1->tensorflow_text) (24.0)\n","Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.17,>=2.16.1->tensorflow_text) (3.20.3)\n","Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.17,>=2.16.1->tensorflow_text) (2.31.0)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.17,>=2.16.1->tensorflow_text) (67.7.2)\n","Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.17,>=2.16.1->tensorflow_text) (1.16.0)\n","Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.17,>=2.16.1->tensorflow_text) (2.4.0)\n","Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.17,>=2.16.1->tensorflow_text) (4.11.0)\n","Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.17,>=2.16.1->tensorflow_text) (1.14.1)\n","Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.17,>=2.16.1->tensorflow_text) (1.64.0)\n","Collecting tensorboard<2.17,>=2.16 (from tensorflow<2.17,>=2.16.1->tensorflow_text)\n","  Downloading tensorboard-2.16.2-py3-none-any.whl (5.5 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.5/5.5 MB\u001b[0m \u001b[31m55.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting keras>=3.0.0 (from tensorflow<2.17,>=2.16.1->tensorflow_text)\n","  Downloading keras-3.3.3-py3-none-any.whl (1.1 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m53.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.17,>=2.16.1->tensorflow_text) (0.37.0)\n","Requirement already satisfied: numpy<2.0.0,>=1.23.5 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.17,>=2.16.1->tensorflow_text) (1.25.2)\n","Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow<2.17,>=2.16.1->tensorflow_text) (0.43.0)\n","Requirement already satisfied: rich in /usr/local/lib/python3.10/dist-packages (from keras>=3.0.0->tensorflow<2.17,>=2.16.1->tensorflow_text) (13.7.1)\n","Collecting namex (from keras>=3.0.0->tensorflow<2.17,>=2.16.1->tensorflow_text)\n","  Downloading namex-0.0.8-py3-none-any.whl (5.8 kB)\n","Collecting optree (from keras>=3.0.0->tensorflow<2.17,>=2.16.1->tensorflow_text)\n","  Downloading optree-0.11.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (311 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m311.2/311.2 kB\u001b[0m \u001b[31m35.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow<2.17,>=2.16.1->tensorflow_text) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow<2.17,>=2.16.1->tensorflow_text) (3.7)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow<2.17,>=2.16.1->tensorflow_text) (2.0.7)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow<2.17,>=2.16.1->tensorflow_text) (2024.2.2)\n","Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.17,>=2.16->tensorflow<2.17,>=2.16.1->tensorflow_text) (3.6)\n","Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.17,>=2.16->tensorflow<2.17,>=2.16.1->tensorflow_text) (0.7.2)\n","Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.17,>=2.16->tensorflow<2.17,>=2.16.1->tensorflow_text) (3.0.3)\n","Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard<2.17,>=2.16->tensorflow<2.17,>=2.16.1->tensorflow_text) (2.1.5)\n","Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich->keras>=3.0.0->tensorflow<2.17,>=2.16.1->tensorflow_text) (3.0.0)\n","Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich->keras>=3.0.0->tensorflow<2.17,>=2.16.1->tensorflow_text) (2.16.1)\n","Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich->keras>=3.0.0->tensorflow<2.17,>=2.16.1->tensorflow_text) (0.1.2)\n","Installing collected packages: namex, optree, ml-dtypes, h5py, tensorboard, keras, tensorflow, tensorflow_text\n","  Attempting uninstall: ml-dtypes\n","    Found existing installation: ml-dtypes 0.2.0\n","    Uninstalling ml-dtypes-0.2.0:\n","      Successfully uninstalled ml-dtypes-0.2.0\n","  Attempting uninstall: h5py\n","    Found existing installation: h5py 3.9.0\n","    Uninstalling h5py-3.9.0:\n","      Successfully uninstalled h5py-3.9.0\n","  Attempting uninstall: tensorboard\n","    Found existing installation: tensorboard 2.15.2\n","    Uninstalling tensorboard-2.15.2:\n","      Successfully uninstalled tensorboard-2.15.2\n","  Attempting uninstall: keras\n","    Found existing installation: keras 2.15.0\n","    Uninstalling keras-2.15.0:\n","      Successfully uninstalled keras-2.15.0\n","  Attempting uninstall: tensorflow\n","    Found existing installation: tensorflow 2.15.0\n","    Uninstalling tensorflow-2.15.0:\n","      Successfully uninstalled tensorflow-2.15.0\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","tf-keras 2.15.1 requires tensorflow<2.16,>=2.15, but you have tensorflow 2.16.1 which is incompatible.\u001b[0m\u001b[31m\n","\u001b[0mSuccessfully installed h5py-3.11.0 keras-3.3.3 ml-dtypes-0.3.2 namex-0.0.8 optree-0.11.0 tensorboard-2.16.2 tensorflow-2.16.1 tensorflow_text-2.16.1\n"]},{"data":{"application/vnd.colab-display-data+json":{"id":"0d7d57d1ba2549f3b3b93519a414921d","pip_warning":{"packages":["h5py","keras","ml_dtypes","tensorboard","tensorflow"]}}},"metadata":{},"output_type":"display_data"}],"source":["pip install tensorflow_text"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":403},"executionInfo":{"elapsed":11,"status":"error","timestamp":1717399238722,"user":{"displayName":"Akbar Maulana Ibrahim M004D4KY3332","userId":"11704943035037494098"},"user_tz":-420},"id":"TEUH3kAD9ZLx","outputId":"6a16c0e8-38af-4141-e6f4-4a816351bf3b"},"outputs":[{"ename":"ModuleNotFoundError","evalue":"No module named 'annoy'","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)","\u001b[0;32m<ipython-input-73-e2d3899358ef>\u001b[0m in \u001b[0;36m<cell line: 5>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtensorflow_hub\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mhub\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mannoy\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mAnnoyIndex\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcluster\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mKMeans\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'annoy'","","\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"]}],"source":["import pandas as pd\n","import numpy as np\n","import tensorflow as tf\n","import tensorflow_hub as hub\n","from annoy import AnnoyIndex\n","from sklearn.cluster import KMeans\n","\n","# Load data\n","def load_data(file_path):\n","    data = pd.read_csv(file_path)\n","    data = data[['Label', 'Title', 'Harga', 'Asal Kota']]  # Selecting the relevant columns\n","    return data\n","\n","# Preprocess the data\n","def preprocess(data):\n","    data['Title'] = data['Title'].str.lower().str.replace('[^\\w\\s]', '', regex=True)\n","    data['Asal Kota'] = data['Asal Kota'].str.lower().str.replace('[^\\w\\s]', '', regex=True)\n","    return data\n","\n","# Load the pre-trained model from TensorFlow Hub\n","embed = hub.load(\"https://tfhub.dev/google/universal-sentence-encoder/4\")\n","\n","# Generate embeddings\n","def get_embeddings(data):\n","    embeddings = embed(data['Title'])\n","    return embeddings.numpy()\n","\n","# Build Annoy index for efficient similarity search\n","def build_index(embeddings):\n","    dimension = embeddings.shape[1]  # Dimensions of embeddings\n","    index = AnnoyIndex(dimension, 'angular')\n","    for i, vector in enumerate(embeddings):\n","        index.add_item(i, vector)\n","    index.build(10)  # More trees, more precision\n","    return index\n","\n","# Cluster data for recommendations\n","def cluster_data(embeddings, num_clusters=10):\n","    kmeans = KMeans(n_clusters=num_clusters, random_state=0).fit(embeddings)\n","    return kmeans.labels_\n","\n","# Search and recommend products\n","def search_and_recommend(query, index, data, embeddings, kmeans_labels, num_results=10):\n","    query_embedding = embed([query]).numpy()[0]\n","    indices = index.get_nns_by_vector(query_embedding, num_results)\n","    primary_results = data.iloc[indices]\n","\n","    # Recommend additional items from the same cluster\n","    if not primary_results.empty:\n","        cluster_label = primary_results['Cluster'].mode()[0]\n","        additional_suggestions = data[data['Cluster'] == cluster_label].sample(n=5)\n","        return pd.concat([primary_results, additional_suggestions]).drop_duplicates()\n","    return primary_results"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"S9iGggng_SIf"},"outputs":[],"source":["# Main function to execute the process\n","def main(file_path):\n","    data = load_data(file_path)\n","    data = preprocess(data)\n","    embeddings = get_embeddings(data)\n","    index = build_index(embeddings)\n","    data['Cluster'] = cluster_data(embeddings)\n","\n","    # Example search\n","    query = \"kasur kos\"\n","    results = search_and_recommend(query, index, data, embeddings, data['Cluster'])\n","    return results"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":576},"executionInfo":{"elapsed":5534,"status":"ok","timestamp":1717081439370,"user":{"displayName":"Akbar Maulana Ibrahim M004D4KY3332","userId":"11704943035037494098"},"user_tz":-420},"id":"qUjf5t49-1rj","outputId":"84869796-6244-4ae7-f47b-6949c368510f"},"outputs":[{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.10/dist-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n","  warnings.warn(\n"]},{"data":{"application/vnd.google.colaboratory.intrinsic+json":{"summary":"{\n  \"name\": \"main('/content/shopee\",\n  \"rows\": 15,\n  \"fields\": [\n    {\n      \"column\": \"Label\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 4,\n        \"samples\": [\n          \"Kipas Angin\",\n          \"Lemari\",\n          \"Kasur\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Title\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 15,\n        \"samples\": [\n          \"miyako ei1008mbk  setrika 320 watt\",\n          \"setrika miyako el1000m el1008m el 2000original sni\",\n          \"kasur lantai mollorca 100cmkasurlipatkasurvakumkasurkorea\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Harga\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 15,\n        \"samples\": [\n          \"102900\",\n          \"95500\",\n          \"226.320\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Asal Kota\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 10,\n        \"samples\": [\n          \"tangerang\",\n          \"surakarta solo\",\n          \"surabaya\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Cluster\",\n      \"properties\": {\n        \"dtype\": \"int32\",\n        \"num_unique_values\": 4,\n        \"samples\": [\n          5,\n          2,\n          9\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}","type":"dataframe"},"text/html":["\n","  <div id=\"df-4255e26d-0394-4217-bd56-ba37ad5e1a8f\" class=\"colab-df-container\">\n","    <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Label</th>\n","      <th>Title</th>\n","      <th>Harga</th>\n","      <th>Asal Kota</th>\n","      <th>Cluster</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>423</th>\n","      <td>Kasur</td>\n","      <td>kasur lantai mollorca 100cmkasurlipatkasurvaku...</td>\n","      <td>226.320</td>\n","      <td>kab bekasi</td>\n","      <td>9</td>\n","    </tr>\n","    <tr>\n","      <th>671</th>\n","      <td>Kipas Angin</td>\n","      <td>1c2 kipas angin portable</td>\n","      <td>18.999</td>\n","      <td>surakarta solo</td>\n","      <td>5</td>\n","    </tr>\n","    <tr>\n","      <th>345</th>\n","      <td>Kasur</td>\n","      <td>kasur dewasa</td>\n","      <td>185.000</td>\n","      <td>kab tangerang</td>\n","      <td>9</td>\n","    </tr>\n","    <tr>\n","      <th>379</th>\n","      <td>Kasur</td>\n","      <td>kasur busa d26 super awet ukrn 180x90x10cm</td>\n","      <td>172.000</td>\n","      <td>kab tangerang</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>1301</th>\n","      <td>Setrika</td>\n","      <td>maspion setrika  iron ex1010</td>\n","      <td>115000</td>\n","      <td>bandung</td>\n","      <td>9</td>\n","    </tr>\n","    <tr>\n","      <th>785</th>\n","      <td>Lemari</td>\n","      <td>lemari portable 4pintu</td>\n","      <td>150000</td>\n","      <td>denpasar</td>\n","      <td>5</td>\n","    </tr>\n","    <tr>\n","      <th>354</th>\n","      <td>Kasur</td>\n","      <td>elephant kasur lantai rebounded</td>\n","      <td>299.000</td>\n","      <td>kab tangerang</td>\n","      <td>9</td>\n","    </tr>\n","    <tr>\n","      <th>1250</th>\n","      <td>Setrika</td>\n","      <td>qq setrika anti lengket</td>\n","      <td>86200</td>\n","      <td>surabaya</td>\n","      <td>9</td>\n","    </tr>\n","    <tr>\n","      <th>1258</th>\n","      <td>Setrika</td>\n","      <td>setrikagosokan maspion ex1010 setrika  hitam</td>\n","      <td>113000</td>\n","      <td>semarang</td>\n","      <td>9</td>\n","    </tr>\n","    <tr>\n","      <th>1107</th>\n","      <td>Setrika</td>\n","      <td>miyako ei1008mbk  setrika 320 watt</td>\n","      <td>102900</td>\n","      <td>surabaya</td>\n","      <td>2</td>\n","    </tr>\n","    <tr>\n","      <th>1307</th>\n","      <td>Setrika</td>\n","      <td>maspion setrika listrik ha365 hijab series dry...</td>\n","      <td>139500</td>\n","      <td>kab sidoarjo</td>\n","      <td>9</td>\n","    </tr>\n","    <tr>\n","      <th>1146</th>\n","      <td>Setrika</td>\n","      <td>setrika miyako el1000m el1008m el 2000original...</td>\n","      <td>95500</td>\n","      <td>tangerang</td>\n","      <td>9</td>\n","    </tr>\n","    <tr>\n","      <th>645</th>\n","      <td>Kipas Angin</td>\n","      <td>gel pendingin kipas angin model ac</td>\n","      <td>9.375</td>\n","      <td>kab bekasi</td>\n","      <td>9</td>\n","    </tr>\n","    <tr>\n","      <th>607</th>\n","      <td>Kipas Angin</td>\n","      <td>kipas angin kipas gantung jumbo lavela</td>\n","      <td>62.500</td>\n","      <td>jakarta timur</td>\n","      <td>9</td>\n","    </tr>\n","    <tr>\n","      <th>524</th>\n","      <td>Kipas Angin</td>\n","      <td>kipas angin 9inch</td>\n","      <td>100.000</td>\n","      <td>denpasar</td>\n","      <td>9</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","    <div class=\"colab-df-buttons\">\n","\n","  <div class=\"colab-df-container\">\n","    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-4255e26d-0394-4217-bd56-ba37ad5e1a8f')\"\n","            title=\"Convert this dataframe to an interactive table.\"\n","            style=\"display:none;\">\n","\n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n","    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n","  </svg>\n","    </button>\n","\n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    .colab-df-buttons div {\n","      margin-bottom: 4px;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","    <script>\n","      const buttonEl =\n","        document.querySelector('#df-4255e26d-0394-4217-bd56-ba37ad5e1a8f button.colab-df-convert');\n","      buttonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","      async function convertToInteractive(key) {\n","        const element = document.querySelector('#df-4255e26d-0394-4217-bd56-ba37ad5e1a8f');\n","        const dataTable =\n","          await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                    [key], {});\n","        if (!dataTable) return;\n","\n","        const docLinkHtml = 'Like what you see? Visit the ' +\n","          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","          + ' to learn more about interactive tables.';\n","        element.innerHTML = '';\n","        dataTable['output_type'] = 'display_data';\n","        await google.colab.output.renderOutput(dataTable, element);\n","        const docLink = document.createElement('div');\n","        docLink.innerHTML = docLinkHtml;\n","        element.appendChild(docLink);\n","      }\n","    </script>\n","  </div>\n","\n","\n","<div id=\"df-ae973aeb-f451-4ebd-935f-4bab4195c1c2\">\n","  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-ae973aeb-f451-4ebd-935f-4bab4195c1c2')\"\n","            title=\"Suggest charts\"\n","            style=\"display:none;\">\n","\n","<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","     width=\"24px\">\n","    <g>\n","        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n","    </g>\n","</svg>\n","  </button>\n","\n","<style>\n","  .colab-df-quickchart {\n","      --bg-color: #E8F0FE;\n","      --fill-color: #1967D2;\n","      --hover-bg-color: #E2EBFA;\n","      --hover-fill-color: #174EA6;\n","      --disabled-fill-color: #AAA;\n","      --disabled-bg-color: #DDD;\n","  }\n","\n","  [theme=dark] .colab-df-quickchart {\n","      --bg-color: #3B4455;\n","      --fill-color: #D2E3FC;\n","      --hover-bg-color: #434B5C;\n","      --hover-fill-color: #FFFFFF;\n","      --disabled-bg-color: #3B4455;\n","      --disabled-fill-color: #666;\n","  }\n","\n","  .colab-df-quickchart {\n","    background-color: var(--bg-color);\n","    border: none;\n","    border-radius: 50%;\n","    cursor: pointer;\n","    display: none;\n","    fill: var(--fill-color);\n","    height: 32px;\n","    padding: 0;\n","    width: 32px;\n","  }\n","\n","  .colab-df-quickchart:hover {\n","    background-color: var(--hover-bg-color);\n","    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n","    fill: var(--button-hover-fill-color);\n","  }\n","\n","  .colab-df-quickchart-complete:disabled,\n","  .colab-df-quickchart-complete:disabled:hover {\n","    background-color: var(--disabled-bg-color);\n","    fill: var(--disabled-fill-color);\n","    box-shadow: none;\n","  }\n","\n","  .colab-df-spinner {\n","    border: 2px solid var(--fill-color);\n","    border-color: transparent;\n","    border-bottom-color: var(--fill-color);\n","    animation:\n","      spin 1s steps(1) infinite;\n","  }\n","\n","  @keyframes spin {\n","    0% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","      border-left-color: var(--fill-color);\n","    }\n","    20% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    30% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","      border-right-color: var(--fill-color);\n","    }\n","    40% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    60% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","    }\n","    80% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-bottom-color: var(--fill-color);\n","    }\n","    90% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","    }\n","  }\n","</style>\n","\n","  <script>\n","    async function quickchart(key) {\n","      const quickchartButtonEl =\n","        document.querySelector('#' + key + ' button');\n","      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n","      quickchartButtonEl.classList.add('colab-df-spinner');\n","      try {\n","        const charts = await google.colab.kernel.invokeFunction(\n","            'suggestCharts', [key], {});\n","      } catch (error) {\n","        console.error('Error during call to suggestCharts:', error);\n","      }\n","      quickchartButtonEl.classList.remove('colab-df-spinner');\n","      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n","    }\n","    (() => {\n","      let quickchartButtonEl =\n","        document.querySelector('#df-ae973aeb-f451-4ebd-935f-4bab4195c1c2 button');\n","      quickchartButtonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","    })();\n","  </script>\n","</div>\n","    </div>\n","  </div>\n"],"text/plain":["            Label                                              Title    Harga  \\\n","423         Kasur  kasur lantai mollorca 100cmkasurlipatkasurvaku...  226.320   \n","671   Kipas Angin                           1c2 kipas angin portable   18.999   \n","345         Kasur                                       kasur dewasa  185.000   \n","379         Kasur         kasur busa d26 super awet ukrn 180x90x10cm  172.000   \n","1301      Setrika                       maspion setrika  iron ex1010   115000   \n","785        Lemari                             lemari portable 4pintu   150000   \n","354         Kasur                    elephant kasur lantai rebounded  299.000   \n","1250      Setrika                            qq setrika anti lengket    86200   \n","1258      Setrika       setrikagosokan maspion ex1010 setrika  hitam   113000   \n","1107      Setrika                 miyako ei1008mbk  setrika 320 watt   102900   \n","1307      Setrika  maspion setrika listrik ha365 hijab series dry...   139500   \n","1146      Setrika  setrika miyako el1000m el1008m el 2000original...    95500   \n","645   Kipas Angin                 gel pendingin kipas angin model ac    9.375   \n","607   Kipas Angin             kipas angin kipas gantung jumbo lavela   62.500   \n","524   Kipas Angin                                  kipas angin 9inch  100.000   \n","\n","           Asal Kota  Cluster  \n","423       kab bekasi        9  \n","671   surakarta solo        5  \n","345    kab tangerang        9  \n","379    kab tangerang        0  \n","1301         bandung        9  \n","785         denpasar        5  \n","354    kab tangerang        9  \n","1250        surabaya        9  \n","1258        semarang        9  \n","1107        surabaya        2  \n","1307    kab sidoarjo        9  \n","1146       tangerang        9  \n","645       kab bekasi        9  \n","607    jakarta timur        9  \n","524         denpasar        9  "]},"execution_count":132,"metadata":{},"output_type":"execute_result"}],"source":["# Hasil Rekomendasi\n","main('/content/shopee.csv')"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":83341,"status":"ok","timestamp":1717397412825,"user":{"displayName":"Akbar Maulana Ibrahim M004D4KY3332","userId":"11704943035037494098"},"user_tz":-420},"id":"3vplR73EjiJU","outputId":"dba456fd-54a4-420c-86b5-9c763ef18ae5"},"outputs":[{"name":"stdout","output_type":"stream","text":["Epoch 1/10\n","1352/1352 [==============================] - 4s 2ms/step - loss: 1.5175 - accuracy: 0.4541\n","Epoch 2/10\n","1352/1352 [==============================] - 3s 2ms/step - loss: 0.5458 - accuracy: 0.8787\n","Epoch 3/10\n","1352/1352 [==============================] - 10s 7ms/step - loss: 0.1500 - accuracy: 0.9933\n","Epoch 4/10\n","1352/1352 [==============================] - 9s 7ms/step - loss: 0.0602 - accuracy: 0.9985\n","Epoch 5/10\n","1352/1352 [==============================] - 10s 8ms/step - loss: 0.0342 - accuracy: 0.9985\n","Epoch 6/10\n","1352/1352 [==============================] - 3s 2ms/step - loss: 0.0230 - accuracy: 0.9993\n","Epoch 7/10\n","1352/1352 [==============================] - 4s 3ms/step - loss: 0.0179 - accuracy: 0.9985\n","Epoch 8/10\n","1352/1352 [==============================] - 4s 3ms/step - loss: 0.0146 - accuracy: 0.9993\n","Epoch 9/10\n","1352/1352 [==============================] - 3s 3ms/step - loss: 0.0111 - accuracy: 0.9993\n","Epoch 10/10\n","1352/1352 [==============================] - 4s 3ms/step - loss: 0.0097 - accuracy: 0.9985\n"]},{"name":"stderr","output_type":"stream","text":["WARNING:tensorflow:6 out of the last 6 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7d937fdbf5b0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"]},{"name":"stdout","output_type":"stream","text":["1/1 [==============================] - 0s 70ms/step\n","[('Setrika', 5.112063e-05), ('Rice Cooker', 0.018218761), ('Lemari', 0.023003256), ('Kipas Angin', 0.042095467), ('Kasur', 0.04556349), ('Meja', 0.87106794)]\n"]}],"source":["import tensorflow as tf\n","from tensorflow.keras.preprocessing.text import Tokenizer\n","from tensorflow.keras.preprocessing.sequence import pad_sequences\n","import numpy as np\n","\n","# Sample data (product titles and corresponding recommendations)\n","product_titles = data['Title']\n","\n","recommendations = list(data['Label'])\n","\n","# Tokenize product titles\n","tokenizer = Tokenizer()\n","tokenizer.fit_on_texts(product_titles)\n","sequences = tokenizer.texts_to_sequences(product_titles)\n","\n","# Padding sequences\n","max_sequence_length = max([len(seq) for seq in sequences])\n","padded_sequences = pad_sequences(sequences, maxlen=max_sequence_length, padding='post')\n","\n","# Convert recommendations to one-hot encoding\n","recommendation_classes = sorted(list(set(recommendations)))\n","recommendations_one_hot = np.zeros((len(recommendations), len(recommendation_classes)))\n","for i, rec in enumerate(recommendations):\n","    idx = recommendation_classes.index(rec)\n","    recommendations_one_hot[i, idx] = 1\n","\n","# Build a simple Sequential model\n","model = tf.keras.Sequential([\n","    tf.keras.layers.Embedding(input_dim=len(tokenizer.word_index) + 1, output_dim=16, input_length=max_sequence_length),\n","    tf.keras.layers.GlobalAveragePooling1D(),\n","    tf.keras.layers.Dense(16, activation='relu'),\n","    tf.keras.layers.Dense(len(recommendation_classes), activation='softmax')\n","])\n","\n","# Compile the model\n","model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n","\n","# Train the model\n","model.fit(padded_sequences, recommendations_one_hot, epochs=10, batch_size=1)\n","\n","# Predict recommendations for new product titles\n","new_product_titles = [\"Meja Belajar Anak\"]\n","new_sequences = tokenizer.texts_to_sequences(new_product_titles)\n","new_padded_sequences = pad_sequences(new_sequences, maxlen=max_sequence_length, padding='post')\n","predictions = model.predict(new_padded_sequences)\n","\n","# Get the top 20 predictions\n","top_indices = np.argsort(predictions, axis=1)[:, -20:][0]\n","top_predictions = predictions[0][top_indices]\n","\n","# Output the top 20 predictions\n","output_predictions = []\n","for i, idx in enumerate(top_indices):\n","    output_predictions.append((recommendation_classes[idx], top_predictions[i]))\n","\n","print(output_predictions)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":571,"status":"ok","timestamp":1717397426059,"user":{"displayName":"Akbar Maulana Ibrahim M004D4KY3332","userId":"11704943035037494098"},"user_tz":-420},"id":"mE-FTw9esjAX","outputId":"5e26e970-3d0e-4159-9bb2-88daf6e2836f"},"outputs":[{"name":"stdout","output_type":"stream","text":["Recommended Product: Meja\n"]}],"source":["predicted_class_index = np.argmax(predictions)\n","\n","# Use the predicted class index to recommend a product\n","recommended_product = recommendations[predicted_class_index]\n","\n","print(\"Recommended Product:\", recommended_product)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":430},"executionInfo":{"elapsed":605,"status":"error","timestamp":1717397741030,"user":{"displayName":"Akbar Maulana Ibrahim M004D4KY3332","userId":"11704943035037494098"},"user_tz":-420},"id":"EUJXRYme2BvR","outputId":"7804bc1c-8b66-4a2f-a41b-9b30fef79ff5"},"outputs":[{"ename":"ValueError","evalue":"y contains previously unseen labels: 'MEJA LIPAT / MEJA LIPAT BELAJAR LAPTOP SERBAGUNA PORTABLE/MEJA BELAJAR KARAKTER MURAH / MEJA FURNITURE RUMAH, 60×40×28CM'","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/utils/_encode.py\u001b[0m in \u001b[0;36m_encode\u001b[0;34m(values, uniques, check_unknown)\u001b[0m\n\u001b[1;32m    223\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 224\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0m_map_to_integer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muniques\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    225\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/utils/_encode.py\u001b[0m in \u001b[0;36m_map_to_integer\u001b[0;34m(values, uniques)\u001b[0m\n\u001b[1;32m    163\u001b[0m     \u001b[0mtable\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_nandict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mval\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muniques\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 164\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtable\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mvalues\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    165\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/utils/_encode.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    163\u001b[0m     \u001b[0mtable\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_nandict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mval\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muniques\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 164\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtable\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mvalues\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    165\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/utils/_encode.py\u001b[0m in \u001b[0;36m__missing__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    157\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnan_value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 158\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    159\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyError\u001b[0m: 'MEJA LIPAT / MEJA LIPAT BELAJAR LAPTOP SERBAGUNA PORTABLE/MEJA BELAJAR KARAKTER MURAH / MEJA FURNITURE RUMAH, 60×40×28CM'","\nDuring handling of the above exception, another exception occurred:\n","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","\u001b[0;32m<ipython-input-58-22cb7f466216>\u001b[0m in \u001b[0;36m<cell line: 22>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0mlabel_encoder\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLabelEncoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0my_train_encoded\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlabel_encoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m \u001b[0my_test_encoded\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlabel_encoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Shape of X_train:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/utils/_set_output.py\u001b[0m in \u001b[0;36mwrapped\u001b[0;34m(self, X, *args, **kwargs)\u001b[0m\n\u001b[1;32m    138\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mwraps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    139\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mwrapped\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 140\u001b[0;31m         \u001b[0mdata_to_wrap\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    141\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_to_wrap\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    142\u001b[0m             \u001b[0;31m# only wrap the first output for cross decomposition\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/preprocessing/_label.py\u001b[0m in \u001b[0;36mtransform\u001b[0;34m(self, y)\u001b[0m\n\u001b[1;32m    137\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 139\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_encode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muniques\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclasses_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    140\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0minverse_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/utils/_encode.py\u001b[0m in \u001b[0;36m_encode\u001b[0;34m(values, uniques, check_unknown)\u001b[0m\n\u001b[1;32m    224\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0m_map_to_integer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muniques\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    225\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 226\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"y contains previously unseen labels: {str(e)}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    227\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    228\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcheck_unknown\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mValueError\u001b[0m: y contains previously unseen labels: 'MEJA LIPAT / MEJA LIPAT BELAJAR LAPTOP SERBAGUNA PORTABLE/MEJA BELAJAR KARAKTER MURAH / MEJA FURNITURE RUMAH, 60×40×28CM'"]}],"source":["# Step 1: Load the CSV File\n","df = data\n","df.head()\n","\n","# Step 2: Preprocess Data\n","from tensorflow.keras.preprocessing.text import Tokenizer\n","from tensorflow.keras.preprocessing.sequence import pad_sequences\n","from sklearn.model_selection import train_test_split\n","from sklearn.preprocessing import LabelEncoder\n","\n","tokenizer = Tokenizer(num_words=5000, oov_token='<OOV>')\n","tokenizer.fit_on_texts(df['Title'].values)\n","word_index = tokenizer.word_index\n","\n","sequences = tokenizer.texts_to_sequences(df['Title'].values)\n","padded_sequences = pad_sequences(sequences, padding='post')\n","\n","X_train, X_test, y_train, y_test = train_test_split(padded_sequences, df['Title'].values, test_size=0.2, random_state=42)\n","\n","label_encoder = LabelEncoder()\n","y_train_encoded = label_encoder.fit_transform(y_train)\n","y_test_encoded = label_encoder.transform(y_test)\n","\n","print(\"Shape of X_train:\", X_train.shape)\n","print(\"Shape of y_train_encoded:\", y_train_encoded.shape)\n","print(\"Shape of X_test:\", X_test.shape)\n","print(\"Shape of y_test_encoded:\", y_test_encoded.shape)\n","\n","# Step 3: Build and Train the Model\n","from tensorflow.keras.models import Model\n","from tensorflow.keras.layers import Input, Embedding, LSTM, Dense, Dropout, GlobalAveragePooling1D\n","\n","embedding_dim = 50\n","max_length = len(padded_sequences[0])\n","\n","inputs = Input(shape=(max_length,))\n","x = Embedding(input_dim=5000, output_dim=embedding_dim, input_length=max_length)(inputs)\n","x = LSTM(64, return_sequences=True)(x)\n","x = GlobalAveragePooling1D()(x)\n","x = Dropout(0.5)(x)\n","outputs = Dense(len(label_encoder.classes_), activation='softmax')(x)\n","\n","model = Model(inputs, outputs)\n","model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n","model.summary()\n","\n","history = model.fit(\n","    X_train, y_train_encoded,\n","    epochs=10,\n","    validation_data=(X_test, y_test_encoded),\n","    batch_size=16,\n","    verbose=1\n",")\n","\n","# Step 4: Make Recommendations\n","def recommend_products(title, model, tokenizer, label_encoder, num_recommendations=3):\n","    sequence = tokenizer.texts_to_sequences([title])\n","    padded_sequence = pad_sequences(sequence, maxlen=len(padded_sequences[0]), padding='post')\n","    predictions = model.predict(padded_sequence).flatten()\n","    top_indices = predictions.argsort()[-num_recommendations:][::-1]\n","\n","    recommended_titles = [label_encoder.inverse_transform([i])[0] for i in top_indices]\n","\n","    return recommended_titles\n","\n","# Example: Recommend products for a given title\n","title = \"Meja kerja kayu untuk kantor\"\n","recommendations = recommend_products(title, model, tokenizer, label_encoder)\n","print(\"Recommended product titles for the title '{}': {}\".format(title, recommendations))\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":9510,"status":"ok","timestamp":1717399089184,"user":{"displayName":"Akbar Maulana Ibrahim M004D4KY3332","userId":"11704943035037494098"},"user_tz":-420},"id":"PZ2AmdIZ5rjd","outputId":"aa57b60f-6217-49aa-95cb-3fcf96bf6073"},"outputs":[{"name":"stdout","output_type":"stream","text":["Epoch 1/100\n","17/17 [==============================] - 4s 36ms/step - loss: 532439.3125 - val_loss: 603904.6875\n","Epoch 2/100\n","17/17 [==============================] - 0s 15ms/step - loss: 526303.8125 - val_loss: 588969.0625\n","Epoch 3/100\n","17/17 [==============================] - 0s 16ms/step - loss: 496207.2188 - val_loss: 526636.2500\n","Epoch 4/100\n","17/17 [==============================] - 0s 18ms/step - loss: 399361.9062 - val_loss: 365100.6875\n","Epoch 5/100\n","17/17 [==============================] - 0s 18ms/step - loss: 220428.0312 - val_loss: 168318.5000\n","Epoch 6/100\n","17/17 [==============================] - 0s 18ms/step - loss: 139599.9375 - val_loss: 135989.0938\n","Epoch 7/100\n","17/17 [==============================] - 0s 19ms/step - loss: 124894.5625 - val_loss: 143842.3906\n","Epoch 8/100\n","17/17 [==============================] - 0s 19ms/step - loss: 107623.2266 - val_loss: 141384.9375\n","Epoch 9/100\n","17/17 [==============================] - 0s 21ms/step - loss: 92720.5469 - val_loss: 139479.2500\n","Epoch 10/100\n","17/17 [==============================] - 0s 23ms/step - loss: 74139.5156 - val_loss: 140765.7500\n","Epoch 11/100\n","17/17 [==============================] - 0s 20ms/step - loss: 56676.4180 - val_loss: 142445.9531\n","43/43 [==============================] - 0s 3ms/step\n","Empty DataFrame\n","Columns: [Recommended Product Titles]\n","Index: []\n"]}],"source":["import pandas as pd\n","from sklearn.preprocessing import LabelEncoder, MinMaxScaler\n","from sklearn.model_selection import train_test_split\n","import numpy as np\n","import tensorflow as tf\n","from tensorflow.keras.models import Model\n","from tensorflow.keras.layers import Input, Embedding, Flatten, Dense, Dropout, concatenate\n","from tensorflow.keras.callbacks import EarlyStopping\n","\n","\n","# Encode the product titles and labels\n","title_encoder = LabelEncoder()\n","data['Title_encoded'] = title_encoder.fit_transform(data['Title'])\n","\n","label_encoder = LabelEncoder()\n","data['Label_encoded'] = label_encoder.fit_transform(data['Label'])\n","\n","# Normalize the product IDs\n","scaler = MinMaxScaler()\n","data['Product ID'] = scaler.fit_transform(data['Product ID'].values.reshape(-1, 1))\n","\n","# Prepare arrays\n","product_ids = data['Product ID'].values\n","titles = data['Title_encoded'].values\n","labels = data['Label_encoded'].values\n","\n","# Split data into train and test sets\n","train_product_ids, test_product_ids, train_titles, test_titles, train_labels, test_labels = train_test_split(product_ids, titles, labels, test_size=0.2, random_state=42)\n","\n","# Define inputs\n","input_product = Input(shape=(1,), name='Product')\n","input_title = Input(shape=(1,), name='Title')\n","input_label = Input(shape=(1,), name='Label')\n","\n","# Define embedding layers\n","embedding_product = Embedding(input_dim=len(data['Product ID']) + 1, output_dim=50)(input_product)\n","embedding_title = Embedding(input_dim=len(data['Title_encoded']) + 1, output_dim=50)(input_title)\n","embedding_label = Embedding(input_dim=len(data['Label_encoded']) + 1, output_dim=50)(input_label)\n","\n","# Flatten embedding layers\n","flat_product = Flatten()(embedding_product)\n","flat_title = Flatten()(embedding_title)\n","flat_label = Flatten()(embedding_label)\n","\n","# Concatenate embeddings\n","concat = concatenate([flat_product, flat_title, flat_label])\n","\n","# Add dense layers\n","dense_1 = Dense(256, activation='relu')(concat)\n","dropout_1 = Dropout(0.5)(dense_1)\n","dense_2 = Dense(128, activation='relu')(dropout_1)\n","dropout_2 = Dropout(0.5)(dense_2)\n","output = Dense(1, activation='linear')(dropout_2)\n","\n","# Define the model\n","model = Model(inputs=[input_product, input_title, input_label], outputs=output)\n","\n","# Compile the model\n","model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.001), loss='mean_squared_error')\n","\n","# Reshape the data to fit the model input\n","train_product_ids = train_product_ids.reshape(-1, 1)\n","test_product_ids = test_product_ids.reshape(-1, 1)\n","train_titles = train_titles.reshape(-1, 1)\n","test_titles = test_titles.reshape(-1, 1)\n","train_labels = train_labels.reshape(-1, 1)\n","test_labels = test_labels.reshape(-1, 1)\n","\n","# Define early stopping\n","early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n","\n","# Train the model\n","model.fit([train_product_ids, train_titles, train_labels], train_titles, epochs=100, batch_size=64, verbose=1, validation_data=([test_product_ids, test_titles, test_labels], test_titles), callbacks=[early_stopping])\n","\n","# Function to get product recommendations based on model predictions\n","def get_recommendations(product_id, title, label, model, top_n=10):\n","    # Create an array of product IDs to predict\n","    all_product_ids = np.array(product_ids).reshape(-1, 1)\n","    all_titles = np.array(titles).reshape(-1, 1)\n","    all_labels = np.array(labels).reshape(-1, 1)\n","\n","    # Predict the titles for all products\n","    predicted_titles = model.predict([all_product_ids, all_titles, all_labels]).flatten()\n","\n","    # Get the indices of the top N predictions\n","    top_indices = np.argsort(predicted_titles)[-top_n:]\n","\n","    # Return the top N product IDs\n","    return product_ids[top_indices]\n","\n","# Function to search for products and get recommendations\n","def search_and_recommend(search_term, data, model, top_n=10):\n","    # Filter the products based on the search term\n","    filtered_data = data[data['Title'].str.contains(search_term, case=False, na=False)]\n","\n","    if filtered_data.empty:\n","        return pd.DataFrame(columns=['Recommended Product Titles'])\n","\n","    # Use the first product ID from the filtered results for recommendations\n","    sample_product_id = filtered_data['Product ID'].values[0]\n","    sample_title = filtered_data['Title_encoded'].values[0]\n","    sample_label = filtered_data['Label_encoded'].values[0]\n","    recommended_product_ids = get_recommendations(sample_product_id, sample_title, sample_label, model, top_n=top_n)\n","\n","    # Get the recommended product titles\n","    recommended_products_titles = data[data['Product ID'].isin(recommended_product_ids)]['Title']\n","\n","    # Ensure the recommended products are similar to the search term\n","    recommended_products_titles = recommended_products_titles[recommended_products_titles.str.contains(search_term, case=False, na=False)]\n","\n","    # Create a DataFrame to display the recommended product titles as a column\n","    recommendations_df = pd.DataFrame(recommended_products_titles.tolist(), columns=['Recommended Product Titles'])\n","\n","    return recommendations_df\n","\n","# Example usage: search for \"Meja Belajar\" and get recommendations\n","search_term = \"Meja Belajar\"\n","recommendations_df = search_and_recommend(search_term, data, model)\n","\n","# Display the recommendations\n","print(recommendations_df)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":570,"status":"ok","timestamp":1717399177687,"user":{"displayName":"Akbar Maulana Ibrahim M004D4KY3332","userId":"11704943035037494098"},"user_tz":-420},"id":"-MUQUHsz90zs","outputId":"b38e6087-4606-4c07-b5af-3a8c551e07b6"},"outputs":[{"name":"stdout","output_type":"stream","text":["Recommended Product Titles:\n","1. Meja Belajar Anak / Meja Belajar Lipat / Meja Laptop / Meja Laptop Portable / Meja Belajar Portable / Meja Laptop Jumbo\n","2. Meja lipat laptop jumbo\n","3. MEJA LAPTOP PORTABEL GSF G-10087 JUMBO / MEJA SERBAGUNA / MEJA FURNITURE MURAH / MEJA LAPTOP DAN BELAJAR LIPAT JUMBO\n","4. Meja Belajar Lipat / Meja lipat Portable / Meja laptop Serbaguna Bermacam Motif Kayu\n","5. Meja Portable Laptop Meja Serbaguna\n","6. Meja Laptop uk besar - MEJA LIPAT / MEJA PORTABLE SERBAGUNA 10087 Meja Belajar Portable / Meja Laptop / Meja Lipat Serbaguna\n","7. MEJA LAPTOP PORTABEL GSF G-10087 / MEJA LAPTOP DAN BELAJAR LIPAT JUMBO / MEJA SERBAGUNA / MEJA FURNITURE MURAH\n","8. MEJA LIPAT /MEJA LAPTOP 60x40 /MEJA PORTABLE SERBAGUNA /MEJA KERJA / MEJA BELAJAR\n","9. Meja Laptop Lipat Meja Lipat Laptop Meja Belajar Anak\n","10. Meja Lipat/ Meja Lipat Portable/ Meja Kayu/ Meja Pelatihan Serbaguna/ Meja Kerja/ Meja Belajar/ Meja Kantor Minimalis/ Meja Lipat Multifungsi Z10A1\n"]}],"source":["import pandas as pd\n","from sklearn.feature_extraction.text import TfidfVectorizer\n","from sklearn.metrics.pairwise import cosine_similarity\n","\n","\n","# Define a function to clean the data\n","def clean_data(df):\n","    df = df.dropna(subset=['Title'])  # Drop rows with missing titles\n","    df = df.reset_index(drop=True)    # Reset index after dropping rows\n","    return df\n","\n","# Clean the data\n","data = clean_data(data)\n","\n","# Vectorize the product titles using TF-IDF\n","tfidf_vectorizer = TfidfVectorizer(stop_words='english')\n","tfidf_matrix = tfidf_vectorizer.fit_transform(data['Title'])\n","\n","# Compute the cosine similarity matrix\n","cosine_sim = cosine_similarity(tfidf_matrix, tfidf_matrix)\n","\n","# Function to get product recommendations based on title similarity\n","def get_recommendations(title, cosine_sim=cosine_sim):\n","    # Find the index of the product that matches the title\n","    idx = data[data['Title'].str.contains(title, case=False, na=False)].index[0]\n","\n","    # Get the pairwise similarity scores of all products with the specified product\n","    sim_scores = list(enumerate(cosine_sim[idx]))\n","\n","    # Sort the products based on the similarity scores\n","    sim_scores = sorted(sim_scores, key=lambda x: x[1], reverse=True)\n","\n","    # Get the indices of the top 10 most similar products\n","    sim_scores = sim_scores[1:11]\n","\n","    # Get the product indices\n","    product_indices = [i[0] for i in sim_scores]\n","\n","    # Return the top 10 most similar product titles\n","    return data['Title'].iloc[product_indices]\n","\n","# Example usage: search for \"Meja Belajar\" and get recommendations\n","search_term = \"Meja kayu\"\n","recommendations = get_recommendations(search_term)\n","\n","# Display the recommendations\n","print(\"Recommended Product Titles:\")\n","for i, title in enumerate(recommendations):\n","    print(f\"{i + 1}. {title}\")\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":953,"status":"ok","timestamp":1717399992624,"user":{"displayName":"Akbar Maulana Ibrahim M004D4KY3332","userId":"11704943035037494098"},"user_tz":-420},"id":"aLJkVMpfAnKc","outputId":"e55522a6-4338-4696-86ac-87cb0f280356"},"outputs":[{"name":"stdout","output_type":"stream","text":["Model: \"sequential_14\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," embedding_27 (Embedding)    (None, 53, 50)            115400    \n","                                                                 \n"," flatten_16 (Flatten)        (None, 2650)              0         \n","                                                                 \n"," dense_39 (Dense)            (None, 128)               339328    \n","                                                                 \n"," dense_40 (Dense)            (None, 50)                6450      \n","                                                                 \n","=================================================================\n","Total params: 461178 (1.76 MB)\n","Trainable params: 461178 (1.76 MB)\n","Non-trainable params: 0 (0.00 Byte)\n","_________________________________________________________________\n","43/43 [==============================] - 0s 3ms/step\n"]}],"source":["import pandas as pd\n","import numpy as np\n","import tensorflow as tf\n","from tensorflow.keras.layers import Embedding, Flatten, Dense, Input\n","from tensorflow.keras.models import Model, Sequential\n","from tensorflow.keras.preprocessing.text import Tokenizer\n","from tensorflow.keras.preprocessing.sequence import pad_sequences\n","from sklearn.metrics.pairwise import cosine_similarity\n","\n","# Prepare data\n","titles = data['Title'].tolist()\n","labels = data['Label'].tolist()\n","\n","# Combine title and label for better semantic understanding\n","combined_text = [f\"{label} {title}\" for label, title in zip(labels, titles)]\n","\n","# Tokenize the combined text\n","tokenizer = Tokenizer()\n","tokenizer.fit_on_texts(combined_text)\n","sequences = tokenizer.texts_to_sequences(combined_text)\n","word_index = tokenizer.word_index\n","vocab_size = len(word_index) + 1\n","\n","# Pad sequences\n","max_sequence_length = max(len(seq) for seq in sequences)\n","data_padded = pad_sequences(sequences, maxlen=max_sequence_length, padding='post')\n","\n","# Define the model\n","embedding_dim = 50\n","\n","model = Sequential()\n","model.add(Input(shape=(max_sequence_length,)))\n","model.add(Embedding(input_dim=vocab_size, output_dim=embedding_dim, input_length=max_sequence_length))\n","model.add(Flatten())\n","model.add(Dense(128, activation='relu'))\n","model.add(Dense(embedding_dim, activation='linear'))\n","\n","model.compile(optimizer='adam', loss='mean_squared_error')\n","model.fit\n","model.summary()\n","\n","# Generate embeddings for the product descriptions\n","embeddings = model.predict(data_padded)\n","\n","# Define the search function\n","def semantic_search(query, embeddings, data, tokenizer, model, max_sequence_length, top_k=10):\n","    # Tokenize and pad the query\n","    query_seq = tokenizer.texts_to_sequences([query])\n","    query_padded = pad_sequences(query_seq, maxlen=max_sequence_length, padding='post')\n","\n","    # Generate the embedding for the query\n","    query_embedding = model.predict(query_padded)\n","\n","    # Calculate cosine similarities\n","    similarities = cosine_similarity(query_embedding, embeddings).flatten()\n","\n","    # Get the top_k products\n","    top_k_indices = np.argsort(similarities)[-top_k:][::-1]\n","    results = data.iloc[top_k_indices]\n","    return results\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":378,"status":"ok","timestamp":1717399943430,"user":{"displayName":"Akbar Maulana Ibrahim M004D4KY3332","userId":"11704943035037494098"},"user_tz":-420},"id":"U3kh4PWsAxKW","outputId":"baed3edd-f6d4-4b96-ca77-ce3ddc8853bc"},"outputs":[{"name":"stdout","output_type":"stream","text":["1/1 [==============================] - 0s 40ms/step\n","            Label  Product ID                                Title   Harga  \\\n","785        Lemari    0.581051               lemari portable 4pintu  150000   \n","230          Meja    0.170244                      meja gaming GFR   195.5   \n","345         Kasur    0.255366                         kasur dewasa     185   \n","89           Meja    0.065877              Meja lipat laptop jumbo   199.5   \n","1262      Setrika    0.934123                setrika philips murah  175000   \n","1249      Setrika    0.924500                       SETRIKA MIYAKO  116000   \n","1225      Setrika    0.906736                       Setrika miyako  115000   \n","144          Meja    0.106588           MEJA MINIMALIS RANGKA BESI     318   \n","239          Meja    0.176906               Meja Lipat Polos Warna    54.5   \n","641   Kipas Angin    0.474463               Helifan 3fungsi KYZUKU      75   \n","1335      Setrika    0.988157      MASPION SETRIKA ELECTRIC EX1010  149000   \n","524   Kipas Angin    0.387861                    kipas angin 9inch     100   \n","44           Meja    0.032568         Meja Komputer Laptop Lesehan    86.9   \n","56           Meja    0.041451  Meja Portable Laptop Meja Serbaguna    98.5   \n","793        Lemari    0.586973          LEMARI PLASTIK 3 SUSUN ELCO  210000   \n","1312      Setrika    0.971132          Setrika Murah Miyako padang  110000   \n","862   Rice Cooker    0.638046                Panci magiccom Miyako   40000   \n","1277      Setrika    0.945226             Setrika Maspion EX 1010B  105000   \n","197          Meja    0.145818    Meja Kerja/meja belajar minimalis     107   \n","753        Lemari    0.557365             RAK TV MINIMALIS RAR 121  250000   \n","\n","             Asal Kota  Title_encoded  Label_encoded  \n","785           Denpasar           1248              2  \n","230      Kab. Semarang           1254              3  \n","345     Kab. Tangerang           1232              0  \n","89    Surakarta (Solo)            762              3  \n","1262       Banjarmasin           1269              5  \n","1249            Padang            996              5  \n","1225          Denpasar           1117              5  \n","144        Kab. Kendal            584              3  \n","239   Surakarta (Solo)            737              3  \n","641          Palembang            210              1  \n","1335        Balikpapan            540              5  \n","524           Denpasar           1237              1  \n","44       Jakarta Timur            700              3  \n","56      Kab. Tangerang            747              3  \n","793          Palembang            456              2  \n","1312            Padang           1085              5  \n","862           Surabaya            875              4  \n","1277          Denpasar           1061              5  \n","197     Kab. Tangerang            698              3  \n","753       Kab. Bandung            917              2  \n"]}],"source":["# Example usage\n","query = \"meja\"\n","results = semantic_search(query, embeddings, data, tokenizer, model, max_sequence_length, top_k=20)\n","\n","print(results)"]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}
