{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: tensorflow in c:\\users\\baihaqi\\appdata\\roaming\\python\\python311\\site-packages (2.12.0)\n",
      "Requirement already satisfied: python-dotenv in c:\\users\\baihaqi\\appdata\\roaming\\python\\python311\\site-packages (1.0.1)\n",
      "Requirement already satisfied: pandas in c:\\users\\baihaqi\\appdata\\roaming\\python\\python311\\site-packages (2.0.2)\n",
      "Requirement already satisfied: numpy in c:\\users\\baihaqi\\appdata\\roaming\\python\\python311\\site-packages (1.23.5)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\baihaqi\\appdata\\roaming\\python\\python311\\site-packages (1.5.0)\n",
      "Requirement already satisfied: tensorflow-intel==2.12.0 in c:\\users\\baihaqi\\appdata\\roaming\\python\\python311\\site-packages (from tensorflow) (2.12.0)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in c:\\users\\baihaqi\\appdata\\roaming\\python\\python311\\site-packages (from tensorflow-intel==2.12.0->tensorflow) (2.1.0)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in c:\\users\\baihaqi\\appdata\\roaming\\python\\python311\\site-packages (from tensorflow-intel==2.12.0->tensorflow) (1.6.3)\n",
      "Requirement already satisfied: flatbuffers>=2.0 in c:\\users\\baihaqi\\appdata\\roaming\\python\\python311\\site-packages (from tensorflow-intel==2.12.0->tensorflow) (24.3.25)\n",
      "Requirement already satisfied: gast<=0.4.0,>=0.2.1 in c:\\users\\baihaqi\\appdata\\roaming\\python\\python311\\site-packages (from tensorflow-intel==2.12.0->tensorflow) (0.4.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in c:\\users\\baihaqi\\appdata\\roaming\\python\\python311\\site-packages (from tensorflow-intel==2.12.0->tensorflow) (0.2.0)\n",
      "Requirement already satisfied: h5py>=2.9.0 in c:\\users\\baihaqi\\appdata\\roaming\\python\\python311\\site-packages (from tensorflow-intel==2.12.0->tensorflow) (3.11.0)\n",
      "Requirement already satisfied: jax>=0.3.15 in c:\\users\\baihaqi\\appdata\\roaming\\python\\python311\\site-packages (from tensorflow-intel==2.12.0->tensorflow) (0.4.29)\n",
      "Requirement already satisfied: libclang>=13.0.0 in c:\\users\\baihaqi\\appdata\\roaming\\python\\python311\\site-packages (from tensorflow-intel==2.12.0->tensorflow) (18.1.1)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in c:\\users\\baihaqi\\appdata\\roaming\\python\\python311\\site-packages (from tensorflow-intel==2.12.0->tensorflow) (3.3.0)\n",
      "Requirement already satisfied: packaging in c:\\program files\\python311\\lib\\site-packages (from tensorflow-intel==2.12.0->tensorflow) (23.2)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in c:\\users\\baihaqi\\appdata\\roaming\\python\\python311\\site-packages (from tensorflow-intel==2.12.0->tensorflow) (4.25.3)\n",
      "Requirement already satisfied: setuptools in c:\\program files\\python311\\lib\\site-packages (from tensorflow-intel==2.12.0->tensorflow) (65.5.0)\n",
      "Requirement already satisfied: six>=1.12.0 in c:\\users\\baihaqi\\appdata\\roaming\\python\\python311\\site-packages (from tensorflow-intel==2.12.0->tensorflow) (1.16.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in c:\\users\\baihaqi\\appdata\\roaming\\python\\python311\\site-packages (from tensorflow-intel==2.12.0->tensorflow) (2.4.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in c:\\users\\baihaqi\\appdata\\roaming\\python\\python311\\site-packages (from tensorflow-intel==2.12.0->tensorflow) (4.11.0)\n",
      "Requirement already satisfied: wrapt<1.15,>=1.11.0 in c:\\users\\baihaqi\\appdata\\roaming\\python\\python311\\site-packages (from tensorflow-intel==2.12.0->tensorflow) (1.14.1)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in c:\\users\\baihaqi\\appdata\\roaming\\python\\python311\\site-packages (from tensorflow-intel==2.12.0->tensorflow) (1.64.1)\n",
      "Requirement already satisfied: tensorboard<2.13,>=2.12 in c:\\users\\baihaqi\\appdata\\roaming\\python\\python311\\site-packages (from tensorflow-intel==2.12.0->tensorflow) (2.12.3)\n",
      "Requirement already satisfied: tensorflow-estimator<2.13,>=2.12.0 in c:\\users\\baihaqi\\appdata\\roaming\\python\\python311\\site-packages (from tensorflow-intel==2.12.0->tensorflow) (2.12.0)\n",
      "Requirement already satisfied: keras<2.13,>=2.12.0 in c:\\users\\baihaqi\\appdata\\roaming\\python\\python311\\site-packages (from tensorflow-intel==2.12.0->tensorflow) (2.12.0)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in c:\\users\\baihaqi\\appdata\\roaming\\python\\python311\\site-packages (from tensorflow-intel==2.12.0->tensorflow) (0.31.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\baihaqi\\appdata\\roaming\\python\\python311\\site-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\baihaqi\\appdata\\roaming\\python\\python311\\site-packages (from pandas) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in c:\\users\\baihaqi\\appdata\\roaming\\python\\python311\\site-packages (from pandas) (2024.1)\n",
      "Requirement already satisfied: scipy>=1.6.0 in c:\\users\\baihaqi\\appdata\\roaming\\python\\python311\\site-packages (from scikit-learn) (1.13.1)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\users\\baihaqi\\appdata\\roaming\\python\\python311\\site-packages (from scikit-learn) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\baihaqi\\appdata\\roaming\\python\\python311\\site-packages (from scikit-learn) (3.5.0)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in c:\\users\\baihaqi\\appdata\\roaming\\python\\python311\\site-packages (from astunparse>=1.6.0->tensorflow-intel==2.12.0->tensorflow) (0.43.0)\n",
      "Requirement already satisfied: ml-dtypes>=0.4.0 in c:\\users\\baihaqi\\appdata\\roaming\\python\\python311\\site-packages (from jax>=0.3.15->tensorflow-intel==2.12.0->tensorflow) (0.4.0)\n",
      "Requirement already satisfied: google-auth<3,>=1.6.3 in c:\\users\\baihaqi\\appdata\\roaming\\python\\python311\\site-packages (from tensorboard<2.13,>=2.12->tensorflow-intel==2.12.0->tensorflow) (2.30.0)\n",
      "Requirement already satisfied: google-auth-oauthlib<1.1,>=0.5 in c:\\users\\baihaqi\\appdata\\roaming\\python\\python311\\site-packages (from tensorboard<2.13,>=2.12->tensorflow-intel==2.12.0->tensorflow) (1.0.0)\n",
      "Requirement already satisfied: markdown>=2.6.8 in c:\\users\\baihaqi\\appdata\\roaming\\python\\python311\\site-packages (from tensorboard<2.13,>=2.12->tensorflow-intel==2.12.0->tensorflow) (3.6)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in c:\\users\\baihaqi\\appdata\\roaming\\python\\python311\\site-packages (from tensorboard<2.13,>=2.12->tensorflow-intel==2.12.0->tensorflow) (2.31.0)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in c:\\users\\baihaqi\\appdata\\roaming\\python\\python311\\site-packages (from tensorboard<2.13,>=2.12->tensorflow-intel==2.12.0->tensorflow) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in c:\\users\\baihaqi\\appdata\\roaming\\python\\python311\\site-packages (from tensorboard<2.13,>=2.12->tensorflow-intel==2.12.0->tensorflow) (3.0.3)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in c:\\users\\baihaqi\\appdata\\roaming\\python\\python311\\site-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow-intel==2.12.0->tensorflow) (5.3.3)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in c:\\users\\baihaqi\\appdata\\roaming\\python\\python311\\site-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow-intel==2.12.0->tensorflow) (0.4.0)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in c:\\users\\baihaqi\\appdata\\roaming\\python\\python311\\site-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow-intel==2.12.0->tensorflow) (4.9)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in c:\\users\\baihaqi\\appdata\\roaming\\python\\python311\\site-packages (from google-auth-oauthlib<1.1,>=0.5->tensorboard<2.13,>=2.12->tensorflow-intel==2.12.0->tensorflow) (1.3.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\baihaqi\\appdata\\roaming\\python\\python311\\site-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow-intel==2.12.0->tensorflow) (3.3.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\baihaqi\\appdata\\roaming\\python\\python311\\site-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow-intel==2.12.0->tensorflow) (2.9)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\baihaqi\\appdata\\roaming\\python\\python311\\site-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow-intel==2.12.0->tensorflow) (1.25.8)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\baihaqi\\appdata\\roaming\\python\\python311\\site-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow-intel==2.12.0->tensorflow) (2020.4.5.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in c:\\users\\baihaqi\\appdata\\roaming\\python\\python311\\site-packages (from werkzeug>=1.0.1->tensorboard<2.13,>=2.12->tensorflow-intel==2.12.0->tensorflow) (2.1.3)\n",
      "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in c:\\users\\baihaqi\\appdata\\roaming\\python\\python311\\site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow-intel==2.12.0->tensorflow) (0.6.0)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in c:\\users\\baihaqi\\appdata\\roaming\\python\\python311\\site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<1.1,>=0.5->tensorboard<2.13,>=2.12->tensorflow-intel==2.12.0->tensorflow) (3.2.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install tensorflow python-dotenv pandas numpy scikit-learn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Necessary Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\akbar\\anaconda3\\lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import requests\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Embedding, Flatten, Dot, Dense\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from sklearn.model_selection import train_test_split\n",
    "from dotenv import load_dotenv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data successfully fetched from http://161.97.109.65:3000/api/users\n",
      "                        _id       firstname       lastname         username  \\\n",
      "0  6665e9847aa0dfec0ad43b26         Machine       Learning  machinelearning   \n",
      "1  6665eab57aa0dfec0ad43b2a  DummyFirstname  DummyLastname       dummydata1   \n",
      "2  6665eac87aa0dfec0ad43b2d  DummyFirstname  DummyLastname       dummydata2   \n",
      "3  6665eacc7aa0dfec0ad43b30  DummyFirstname  DummyLastname       dummydata3   \n",
      "4  6665eacf7aa0dfec0ad43b33  DummyFirstname  DummyLastname       dummydata4   \n",
      "\n",
      "                  email         phone  \\\n",
      "0          ml@admin.com    7777777777   \n",
      "1  dummydata1@admin.com  777777770001   \n",
      "2  dummydata2@admin.com  777777770002   \n",
      "3  dummydata3@admin.com  777777770003   \n",
      "4  dummydata4@admin.com  777777770004   \n",
      "\n",
      "                                            password           address  \\\n",
      "0  $2a$10$oNSoSQcmxvHAefk5dKx0UuJw8oSdGeCumA.ZqIN...  Bangkit Capstone   \n",
      "1  $2a$10$ihldsbescWBR9v94/sRhReBpX8mZMGrRpwkUohU...   Dummy Address 1   \n",
      "2  $2a$10$IipmxQztB7MnyyTVUka6n.IK9C/wdqcEf8SXZjD...   Dummy Address 2   \n",
      "3  $2a$10$mQEfWkNV4c6.E6glZRmlyuNzrAVKctoLLXMv2pK...   Dummy Address 3   \n",
      "4  $2a$10$GYFAH4GoxInzAr8WkAeUEuLTYD5ZLjFumTyR2ZP...   Dummy Address 4   \n",
      "\n",
      "                        avatar   role                 createdat  \\\n",
      "0  /uploads/default_avatar.png  buyer  2024-06-09T17:42:28.211Z   \n",
      "1  /uploads/default_avatar.png  buyer  2024-06-09T17:47:33.477Z   \n",
      "2  /uploads/default_avatar.png  buyer  2024-06-09T17:47:52.685Z   \n",
      "3  /uploads/default_avatar.png  buyer  2024-06-09T17:47:56.118Z   \n",
      "4  /uploads/default_avatar.png  buyer  2024-06-09T17:47:59.747Z   \n",
      "\n",
      "                  updatedat ratings  __v  \n",
      "0  2024-06-09T17:42:28.211Z      []    0  \n",
      "1  2024-06-09T17:47:33.477Z      []    0  \n",
      "2  2024-06-09T17:47:52.685Z      []    0  \n",
      "3  2024-06-09T17:47:56.118Z      []    0  \n",
      "4  2024-06-09T17:47:59.747Z      []    0  \n",
      "Data successfully fetched from http://161.97.109.65:3000/api/products\n",
      "                        _id category     price  \\\n",
      "0  6667ef73b3e75416b2fa7e33     Meja  155000.0   \n",
      "1  6667ef73b3e75416b2fa7e34     Meja  124000.0   \n",
      "2  6667ef73b3e75416b2fa7e35     Meja  107000.0   \n",
      "3  6667ef73b3e75416b2fa7e36     Meja   99500.0   \n",
      "4  6667ef73b3e75416b2fa7e37     Meja  446000.0   \n",
      "\n",
      "                                                name  \\\n",
      "0  Damaindah Meja Belajar Kayu Set Kursi / Meja B...   \n",
      "1  Homedoki Meja / Meja Makan / Meja Komputer / M...   \n",
      "2  Sakula Meja kantor meja kerja Meja Komputer Pe...   \n",
      "3  Meja Portable Stand Laptop Meja Laptop Standin...   \n",
      "4  PiPi Furniture Meja Gaming / Meja komputer / M...   \n",
      "\n",
      "                   sellerId  \\\n",
      "0  6665e9847aa0dfec0ad43b26   \n",
      "1  6665e9847aa0dfec0ad43b26   \n",
      "2  6665e9847aa0dfec0ad43b26   \n",
      "3  6665e9847aa0dfec0ad43b26   \n",
      "4  6665e9847aa0dfec0ad43b26   \n",
      "\n",
      "                                        productImage  __v  \n",
      "0  [https://storage.googleapis.com/kelas-app-test...    0  \n",
      "1  [https://storage.googleapis.com/kelas-app-test...    0  \n",
      "2  [https://storage.googleapis.com/kelas-app-test...    0  \n",
      "3  [https://storage.googleapis.com/kelas-app-test...    0  \n",
      "4  [https://storage.googleapis.com/kelas-app-test...    0  \n",
      "Data successfully fetched from http://161.97.109.65:3000/api/interactions\n",
      "                                        interactions\n",
      "0  {'_id': '6667f373b3e75416b2fa83a7', 'userId': ...\n",
      "1  {'_id': '6667f373b3e75416b2fa83a8', 'userId': ...\n",
      "2  {'_id': '6667f373b3e75416b2fa83a9', 'userId': ...\n",
      "3  {'_id': '6667f373b3e75416b2fa83aa', 'userId': ...\n",
      "4  {'_id': '6667f373b3e75416b2fa83ab', 'userId': ...\n",
      "All data fetched successfully.\n"
     ]
    }
   ],
   "source": [
    "load_dotenv()  # Load environment variables from .env file\n",
    "jwt_token = os.getenv('JWT_TOKEN')\n",
    "\n",
    "headers = {'Authorization': f'Bearer {jwt_token}'}\n",
    "\n",
    "api_urls = {\n",
    "    'interactions': 'http://161.97.109.65:3000/api/interactions',\n",
    "    'users': 'http://161.97.109.65:3000/api/users',\n",
    "    'products': 'http://161.97.109.65:3000/api/products'\n",
    "}\n",
    "\n",
    "def fetch_data(url, headers):\n",
    "    try:\n",
    "        response = requests.get(url, headers=headers)\n",
    "        response.raise_for_status()  # Raises an HTTPError for bad requests\n",
    "        data = pd.DataFrame(response.json())\n",
    "        print(f\"Data successfully fetched from {url}\")\n",
    "        print(data.head())  # Display the first few rows of the DataFrame\n",
    "        return data\n",
    "    except requests.RequestException as e:\n",
    "        print(f'Failed to fetch data from {url}: {str(e)}')\n",
    "        return pd.DataFrame()\n",
    "\n",
    "# Fetch data from APIs\n",
    "users = fetch_data(api_urls['users'], headers)\n",
    "products = fetch_data(api_urls['products'], headers)\n",
    "interactions = fetch_data(api_urls['interactions'], headers)\n",
    "\n",
    "# Check if data was fetched successfully\n",
    "if not users.empty and not products.empty and not interactions.empty:\n",
    "    print(\"All data fetched successfully.\")\n",
    "else:\n",
    "    print(\"Data fetching failed, check errors and retry.\")\n",
    "    # Optionally, add logic to halt further processing if data is crucial\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's assume 'interactions' is a DataFrame with a column containing dictionaries\n",
    "# First, ensure that the 'interactions' column is appropriately normalized\n",
    "if 'interactions' in interactions.columns:\n",
    "    interactions_expanded = pd.json_normalize(interactions['interactions'])\n",
    "else:\n",
    "    interactions_expanded = pd.json_normalize(interactions.iloc[:, 0])  # If 'interactions' is the name of DataFrame and not a column\n",
    "\n",
    "# Assuming the JSON data has keys 'userId', 'productId', and 'interactionValue'\n",
    "interactions_expanded['user_id'] = interactions_expanded['userId']\n",
    "interactions_expanded['product_id'] = interactions_expanded['productId']\n",
    "interactions_expanded['interaction_value'] = interactions_expanded['interactionValue']\n",
    "\n",
    "# Encode user_id and product_id\n",
    "user_ids = interactions_expanded['user_id'].unique().tolist()\n",
    "product_ids = interactions_expanded['product_id'].unique().tolist()\n",
    "\n",
    "user2user_encoded = {x: i for i, x in enumerate(user_ids)}\n",
    "product2product_encoded = {x: i for i, x in enumerate(product_ids)}\n",
    "userencoded2user = {i: x for i, x in enumerate(user_ids)}\n",
    "productencoded2product = {i: x for i, x in enumerate(product_ids)}\n",
    "\n",
    "interactions_expanded['user'] = interactions_expanded['user_id'].map(user2user_encoded)\n",
    "interactions_expanded['product'] = interactions_expanded['product_id'].map(product2product_encoded)\n",
    "\n",
    "# Split the data\n",
    "train, test = train_test_split(interactions_expanded, test_size=0.2, random_state=42)\n",
    "\n",
    "# Convert data into required format\n",
    "x_train = [train['user'].values, train['product'].values]\n",
    "y_train = train['interaction_value'].values\n",
    "x_test = [test['user'].values, test['product'].values]\n",
    "y_test = test['interaction_value'].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\akbar\\anaconda3\\lib\\site-packages\\keras\\src\\backend.py:1398: The name tf.executing_eagerly_outside_functions is deprecated. Please use tf.compat.v1.executing_eagerly_outside_functions instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "user_input = Input(shape=(1,))\n",
    "user_embedding = Embedding(len(user2user_encoded), 50)(user_input)  # Removed input_length\n",
    "user_vec = Flatten()(user_embedding)\n",
    "\n",
    "product_input = Input(shape=(1,))\n",
    "product_embedding = Embedding(len(product2product_encoded), 50)(product_input)  # Removed input_length\n",
    "product_vec = Flatten()(product_embedding)\n",
    "\n",
    "dot_product = Dot(axes=1)([user_vec, product_vec])\n",
    "model = Model(inputs=[user_input, product_input], outputs=dot_product)\n",
    "model.compile(optimizer=Adam(), loss='mean_squared_error')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "WARNING:tensorflow:From c:\\Users\\akbar\\anaconda3\\lib\\site-packages\\keras\\src\\utils\\tf_utils.py:492: The name tf.ragged.RaggedTensorValue is deprecated. Please use tf.compat.v1.ragged.RaggedTensorValue instead.\n",
      "\n",
      "40/40 [==============================] - 3s 19ms/step - loss: 4.6985 - val_loss: 4.5376\n",
      "Epoch 2/20\n",
      "40/40 [==============================] - 0s 7ms/step - loss: 4.6649 - val_loss: 4.5351\n",
      "Epoch 3/20\n",
      "40/40 [==============================] - 0s 7ms/step - loss: 4.6095 - val_loss: 4.5301\n",
      "Epoch 4/20\n",
      "40/40 [==============================] - 0s 7ms/step - loss: 4.5027 - val_loss: 4.5188\n",
      "Epoch 5/20\n",
      "40/40 [==============================] - 0s 7ms/step - loss: 4.3157 - val_loss: 4.4987\n",
      "Epoch 6/20\n",
      "40/40 [==============================] - 0s 7ms/step - loss: 4.0297 - val_loss: 4.4691\n",
      "Epoch 7/20\n",
      "40/40 [==============================] - 1s 13ms/step - loss: 3.6390 - val_loss: 4.4289\n",
      "Epoch 8/20\n",
      "40/40 [==============================] - 0s 7ms/step - loss: 3.1686 - val_loss: 4.3788\n",
      "Epoch 9/20\n",
      "40/40 [==============================] - 0s 6ms/step - loss: 2.6553 - val_loss: 4.3256\n",
      "Epoch 10/20\n",
      "40/40 [==============================] - 0s 8ms/step - loss: 2.1445 - val_loss: 4.2737\n",
      "Epoch 11/20\n",
      "40/40 [==============================] - 0s 7ms/step - loss: 1.6704 - val_loss: 4.2244\n",
      "Epoch 12/20\n",
      "40/40 [==============================] - 0s 8ms/step - loss: 1.2632 - val_loss: 4.1801\n",
      "Epoch 13/20\n",
      "40/40 [==============================] - 0s 6ms/step - loss: 0.9261 - val_loss: 4.1465\n",
      "Epoch 14/20\n",
      "40/40 [==============================] - 0s 10ms/step - loss: 0.6625 - val_loss: 4.1193\n",
      "Epoch 15/20\n",
      "40/40 [==============================] - 1s 13ms/step - loss: 0.4683 - val_loss: 4.0975\n",
      "Epoch 16/20\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.3264 - val_loss: 4.0817\n",
      "Epoch 17/20\n",
      "40/40 [==============================] - 0s 9ms/step - loss: 0.2280 - val_loss: 4.0700\n",
      "Epoch 18/20\n",
      "40/40 [==============================] - 0s 6ms/step - loss: 0.1600 - val_loss: 4.0598\n",
      "Epoch 19/20\n",
      "40/40 [==============================] - 0s 6ms/step - loss: 0.1143 - val_loss: 4.0538\n",
      "Epoch 20/20\n",
      "40/40 [==============================] - 0s 6ms/step - loss: 0.0828 - val_loss: 4.0491\n",
      "Epoch 1/20\n",
      "27/40 [===================>..........] - ETA: 0s - loss: 0.0587\n",
      "Epoch 1: val_loss improved from inf to 4.04671, saving model to ..\\model\\collaborative_model.keras\n",
      "40/40 [==============================] - 0s 10ms/step - loss: 0.0618 - val_loss: 4.0467\n",
      "Epoch 2/20\n",
      "34/40 [========================>.....] - ETA: 0s - loss: 0.0456\n",
      "Epoch 2: val_loss improved from 4.04671 to 4.04362, saving model to ..\\model\\collaborative_model.keras\n",
      "40/40 [==============================] - 0s 8ms/step - loss: 0.0473 - val_loss: 4.0436\n",
      "Epoch 3/20\n",
      "27/40 [===================>..........] - ETA: 0s - loss: 0.0361\n",
      "Epoch 3: val_loss improved from 4.04362 to 4.04158, saving model to ..\\model\\collaborative_model.keras\n",
      "40/40 [==============================] - 0s 7ms/step - loss: 0.0375 - val_loss: 4.0416\n",
      "Epoch 4/20\n",
      "33/40 [=======================>......] - ETA: 0s - loss: 0.0325\n",
      "Epoch 4: val_loss improved from 4.04158 to 4.04039, saving model to ..\\model\\collaborative_model.keras\n",
      "40/40 [==============================] - 0s 8ms/step - loss: 0.0305 - val_loss: 4.0404\n",
      "Epoch 5/20\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.0264\n",
      "Epoch 5: val_loss improved from 4.04039 to 4.03951, saving model to ..\\model\\collaborative_model.keras\n",
      "40/40 [==============================] - 0s 9ms/step - loss: 0.0264 - val_loss: 4.0395\n",
      "Epoch 6/20\n",
      "34/40 [========================>.....] - ETA: 0s - loss: 0.0229\n",
      "Epoch 6: val_loss improved from 4.03951 to 4.03834, saving model to ..\\model\\collaborative_model.keras\n",
      "40/40 [==============================] - 0s 12ms/step - loss: 0.0231 - val_loss: 4.0383\n",
      "Epoch 7/20\n",
      "29/40 [====================>.........] - ETA: 0s - loss: 0.0182\n",
      "Epoch 7: val_loss improved from 4.03834 to 4.03790, saving model to ..\\model\\collaborative_model.keras\n",
      "40/40 [==============================] - 0s 7ms/step - loss: 0.0207 - val_loss: 4.0379\n",
      "Epoch 8/20\n",
      "29/40 [====================>.........] - ETA: 0s - loss: 0.0192\n",
      "Epoch 8: val_loss improved from 4.03790 to 4.03693, saving model to ..\\model\\collaborative_model.keras\n",
      "40/40 [==============================] - 0s 8ms/step - loss: 0.0190 - val_loss: 4.0369\n",
      "Epoch 9/20\n",
      "31/40 [======================>.......] - ETA: 0s - loss: 0.0170\n",
      "Epoch 9: val_loss improved from 4.03693 to 4.03631, saving model to ..\\model\\collaborative_model.keras\n",
      "40/40 [==============================] - 0s 10ms/step - loss: 0.0181 - val_loss: 4.0363\n",
      "Epoch 10/20\n",
      "36/40 [==========================>...] - ETA: 0s - loss: 0.0185\n",
      "Epoch 10: val_loss improved from 4.03631 to 4.03573, saving model to ..\\model\\collaborative_model.keras\n",
      "40/40 [==============================] - 0s 8ms/step - loss: 0.0172 - val_loss: 4.0357\n",
      "Epoch 11/20\n",
      "39/40 [============================>.] - ETA: 0s - loss: 0.0166\n",
      "Epoch 11: val_loss improved from 4.03573 to 4.03553, saving model to ..\\model\\collaborative_model.keras\n",
      "40/40 [==============================] - 0s 11ms/step - loss: 0.0164 - val_loss: 4.0355\n",
      "Epoch 12/20\n",
      "31/40 [======================>.......] - ETA: 0s - loss: 0.0145\n",
      "Epoch 12: val_loss improved from 4.03553 to 4.03490, saving model to ..\\model\\collaborative_model.keras\n",
      "40/40 [==============================] - 0s 9ms/step - loss: 0.0159 - val_loss: 4.0349\n",
      "Epoch 13/20\n",
      "29/40 [====================>.........] - ETA: 0s - loss: 0.0173\n",
      "Epoch 13: val_loss improved from 4.03490 to 4.03467, saving model to ..\\model\\collaborative_model.keras\n",
      "40/40 [==============================] - 0s 10ms/step - loss: 0.0155 - val_loss: 4.0347\n",
      "Epoch 14/20\n",
      "26/40 [==================>...........] - ETA: 0s - loss: 0.0144\n",
      "Epoch 14: val_loss improved from 4.03467 to 4.03417, saving model to ..\\model\\collaborative_model.keras\n",
      "40/40 [==============================] - 0s 8ms/step - loss: 0.0152 - val_loss: 4.0342\n",
      "Epoch 15/20\n",
      "33/40 [=======================>......] - ETA: 0s - loss: 0.0158\n",
      "Epoch 15: val_loss improved from 4.03417 to 4.03381, saving model to ..\\model\\collaborative_model.keras\n",
      "40/40 [==============================] - 0s 10ms/step - loss: 0.0148 - val_loss: 4.0338\n",
      "Epoch 16/20\n",
      "35/40 [=========================>....] - ETA: 0s - loss: 0.0128\n",
      "Epoch 16: val_loss improved from 4.03381 to 4.03345, saving model to ..\\model\\collaborative_model.keras\n",
      "40/40 [==============================] - 0s 8ms/step - loss: 0.0147 - val_loss: 4.0335\n",
      "Epoch 17/20\n",
      "35/40 [=========================>....] - ETA: 0s - loss: 0.0156  \n",
      "Epoch 17: val_loss did not improve from 4.03345\n",
      "40/40 [==============================] - 0s 7ms/step - loss: 0.0149 - val_loss: 4.0335\n",
      "Epoch 18/20\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.0146\n",
      "Epoch 18: val_loss improved from 4.03345 to 4.03324, saving model to ..\\model\\collaborative_model.keras\n",
      "40/40 [==============================] - 0s 10ms/step - loss: 0.0146 - val_loss: 4.0332\n",
      "Epoch 19/20\n",
      "39/40 [============================>.] - ETA: 0s - loss: 0.0140 \n",
      "Epoch 19: val_loss improved from 4.03324 to 4.03304, saving model to ..\\model\\collaborative_model.keras\n",
      "40/40 [==============================] - 0s 11ms/step - loss: 0.0147 - val_loss: 4.0330\n",
      "Epoch 20/20\n",
      "39/40 [============================>.] - ETA: 0s - loss: 0.0148\n",
      "Epoch 20: val_loss improved from 4.03304 to 4.03270, saving model to ..\\model\\collaborative_model.keras\n",
      "40/40 [==============================] - 0s 10ms/step - loss: 0.0146 - val_loss: 4.0327\n"
     ]
    }
   ],
   "source": [
    "# Assuming 'train' and 'test' datasets are already split and preprocessed\n",
    "x_train = [np.array(train['user']), np.array(train['product'])]\n",
    "y_train = np.array(train['interaction_value'])\n",
    "\n",
    "# Fit the model\n",
    "history = model.fit(\n",
    "    x_train, \n",
    "    y_train, \n",
    "    epochs=20, \n",
    "    verbose=1, \n",
    "    validation_split=0.2\n",
    ")\n",
    "\n",
    "# Optionally, you can add callbacks, for example to save the best model or early stopping\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "\n",
    "# Define the path to the model directory\n",
    "model_dir = os.path.join('..', 'model', 'collaborative_model.keras')\n",
    "\n",
    "# Setup callbacks\n",
    "callbacks = [\n",
    "    EarlyStopping(monitor='val_loss', patience=5, verbose=1, restore_best_weights=True),\n",
    "    ModelCheckpoint(model_dir, monitor='val_loss', save_best_only=True, verbose=1)\n",
    "]\n",
    "\n",
    "# Fit the model with callbacks\n",
    "history = model.fit(\n",
    "    x_train, \n",
    "    y_train, \n",
    "    epochs=20, \n",
    "    verbose=1, \n",
    "    validation_split=0.2,\n",
    "    callbacks=callbacks\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Try Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33/33 [==============================] - 0s 3ms/step\n",
      "Recommended products for user 6665eab57aa0dfec0ad43b2a:\n",
      "                           _id     category     price  \\\n",
      "102   6667ef73b3e75416b2fa7e99         Meja   62000.0   \n",
      "104   6667ef73b3e75416b2fa7e9b         Meja   67900.0   \n",
      "135   6667ef73b3e75416b2fa7eba         Meja  144250.0   \n",
      "140   6667ef73b3e75416b2fa7ebf         Meja  195000.0   \n",
      "145   6667ef73b3e75416b2fa7ec4         Meja  174982.0   \n",
      "174   6667ef73b3e75416b2fa7ee1         Meja  120000.0   \n",
      "198   6667ef73b3e75416b2fa7ef9         Meja   88000.0   \n",
      "243   6667ef73b3e75416b2fa7f26         Meja  239000.0   \n",
      "300   6667ef73b3e75416b2fa7f5f        Kasur   85400.0   \n",
      "366   6667ef73b3e75416b2fa7fa1        Kasur  139900.0   \n",
      "400   6667ef73b3e75416b2fa7fc3        Kasur  239000.0   \n",
      "409   6667ef73b3e75416b2fa7fcc        Kasur  313900.0   \n",
      "433   6667ef73b3e75416b2fa7fe4  Kipas Angin  199000.0   \n",
      "491   6667ef73b3e75416b2fa801e  Kipas Angin   31500.0   \n",
      "515   6667ef73b3e75416b2fa8036  Kipas Angin  175000.0   \n",
      "542   6667ef73b3e75416b2fa8051  Kipas Angin  139000.0   \n",
      "562   6667ef73b3e75416b2fa8065  Kipas Angin   67999.0   \n",
      "575   6667ef73b3e75416b2fa8072  Kipas Angin  195000.0   \n",
      "576   6667ef73b3e75416b2fa8073  Kipas Angin   59000.0   \n",
      "584   6667ef73b3e75416b2fa807b  Kipas Angin   56000.0   \n",
      "761   6667ef73b3e75416b2fa812c       Lemari  155000.0   \n",
      "860   6667ef73b3e75416b2fa818f  Rice Cooker  193320.0   \n",
      "868   6667ef73b3e75416b2fa8197  Rice Cooker  170000.0   \n",
      "920   6667ef73b3e75416b2fa81cb  Rice Cooker  214000.0   \n",
      "967   6667ef73b3e75416b2fa81fa  Rice Cooker  264000.0   \n",
      "1145  6667ef76b3e75416b2fa82ac      Setrika   67500.0   \n",
      "1240  6667ef76b3e75416b2fa830b      Setrika  119900.0   \n",
      "1273  6667ef76b3e75416b2fa832c      Setrika  100600.0   \n",
      "1319  6667ef76b3e75416b2fa835a      Setrika  290000.0   \n",
      "1344  6667ef76b3e75416b2fa8373      Setrika  134900.0   \n",
      "\n",
      "                                                   name  \\\n",
      "102   Meja Laptop Portable Meja Laptop Roda Meja Lip...   \n",
      "104   Meja komputer laptop Lipat Adjustable Portable...   \n",
      "135   ( Premium ) Meja Lipat Laptop / Meja Laptop Li...   \n",
      "140   Meja Kayu Laptop Komputer Kerja Multifungsi da...   \n",
      "145   Meja Laptop Ruang Kerja | Meja Serbaguna Kaki ...   \n",
      "174           MEJA LAPTOP RANGKA BESI LESEHAN MINIMALIS   \n",
      "198   Meja Belajar Murah Minimalis untuk Anak ataupu...   \n",
      "243   Coffee Table Meja Kopi Meja Tamu Meja Tamu Min...   \n",
      "300   PROMO !!! 85rb Dapat 1pc Kasur Tebal 6cm MUAT ...   \n",
      "366   KASUR LIPAT BUSA INOAC JAPAN QUALITY TEBAL 10C...   \n",
      "400   Kasur Lipat MILLIOM Melon Green Anti Slip | Te...   \n",
      "409   IGOYO Kasur Lipat Navy 90/120/140/160x200cm Ma...   \n",
      "433   Weyon Kipas Angin /Kipas Angin Berdiri Listrik...   \n",
      "491   Kipas Angin USB Bahan Besi USB Mini Fan Merek ...   \n",
      "515   Advance TDS-10 Desk Fan Wall Fan 10\" 2in1 Kipa...   \n",
      "542   Advance Kipas Angin Standfan 16\" SF-16A Advanc...   \n",
      "562   ( FREE BUBBLE ) PROMO!! boxfan kipas angin kar...   \n",
      "575   Nagoya Kipas Angin Dinding Besi 18 Inchi Indus...   \n",
      "576   KIPAS ANGIN MEJA LUCU / DESKFAN 8 INCH Doraemo...   \n",
      "584   Kipas Angin Meja /Desk Fan Advance Karakter Ke...   \n",
      "761   Lemari Plastik 2 3 4 5 Susun NEW DIAMOND Lemar...   \n",
      "860                        Cosmos Rice Cooker CRJ-101 N   \n",
      "868   MIYAKO PSG-607 Multi Cooker 0.63L / Rice Cooke...   \n",
      "920                           Magic Com Cosmos CRJ 1001   \n",
      "967                   Cosmos CRJ-3301 Rice Cooker 1.8 L   \n",
      "1145  SETRIKA LISTRIK - GOSOKAN PAKAIAN / STEAMER Na...   \n",
      "1240  Cosmos CIS-418 Setrika / Gosokan Alas Stainles...   \n",
      "1273                     SETRIKA MASPION X1010/ GOSOKAN   \n",
      "1319  Setrika Philips Classic 1172 Garansi Resmi - H...   \n",
      "1344  Maspion MSP Setrika / Gosokan Listrik HA-365 C...   \n",
      "\n",
      "                      sellerId  \\\n",
      "102   6665e9847aa0dfec0ad43b26   \n",
      "104   6665e9847aa0dfec0ad43b26   \n",
      "135   6665e9847aa0dfec0ad43b26   \n",
      "140   6665e9847aa0dfec0ad43b26   \n",
      "145   6665e9847aa0dfec0ad43b26   \n",
      "174   6665e9847aa0dfec0ad43b26   \n",
      "198   6665e9847aa0dfec0ad43b26   \n",
      "243   6665e9847aa0dfec0ad43b26   \n",
      "300   6665e9847aa0dfec0ad43b26   \n",
      "366   6665e9847aa0dfec0ad43b26   \n",
      "400   6665e9847aa0dfec0ad43b26   \n",
      "409   6665e9847aa0dfec0ad43b26   \n",
      "433   6665e9847aa0dfec0ad43b26   \n",
      "491   6665e9847aa0dfec0ad43b26   \n",
      "515   6665e9847aa0dfec0ad43b26   \n",
      "542   6665e9847aa0dfec0ad43b26   \n",
      "562   6665e9847aa0dfec0ad43b26   \n",
      "575   6665e9847aa0dfec0ad43b26   \n",
      "576   6665e9847aa0dfec0ad43b26   \n",
      "584   6665e9847aa0dfec0ad43b26   \n",
      "761   6665e9847aa0dfec0ad43b26   \n",
      "860   6665e9847aa0dfec0ad43b26   \n",
      "868   6665e9847aa0dfec0ad43b26   \n",
      "920   6665e9847aa0dfec0ad43b26   \n",
      "967   6665e9847aa0dfec0ad43b26   \n",
      "1145  6665e9847aa0dfec0ad43b26   \n",
      "1240  6665e9847aa0dfec0ad43b26   \n",
      "1273  6665e9847aa0dfec0ad43b26   \n",
      "1319  6665e9847aa0dfec0ad43b26   \n",
      "1344  6665e9847aa0dfec0ad43b26   \n",
      "\n",
      "                                           productImage  __v  \n",
      "102   [https://storage.googleapis.com/kelas-app-test...    0  \n",
      "104   [https://storage.googleapis.com/kelas-app-test...    0  \n",
      "135   [https://storage.googleapis.com/kelas-app-test...    0  \n",
      "140   [https://storage.googleapis.com/kelas-app-test...    0  \n",
      "145   [https://storage.googleapis.com/kelas-app-test...    0  \n",
      "174   [https://storage.googleapis.com/kelas-app-test...    0  \n",
      "198   [https://storage.googleapis.com/kelas-app-test...    0  \n",
      "243   [https://storage.googleapis.com/kelas-app-test...    0  \n",
      "300   [https://storage.googleapis.com/kelas-app-test...    0  \n",
      "366   [https://storage.googleapis.com/kelas-app-test...    0  \n",
      "400   [https://storage.googleapis.com/kelas-app-test...    0  \n",
      "409   [https://storage.googleapis.com/kelas-app-test...    0  \n",
      "433   [https://storage.googleapis.com/kelas-app-test...    0  \n",
      "491   [https://storage.googleapis.com/kelas-app-test...    0  \n",
      "515   [https://storage.googleapis.com/kelas-app-test...    0  \n",
      "542   [https://storage.googleapis.com/kelas-app-test...    0  \n",
      "562   [https://storage.googleapis.com/kelas-app-test...    0  \n",
      "575   [https://storage.googleapis.com/kelas-app-test...    0  \n",
      "576   [https://storage.googleapis.com/kelas-app-test...    0  \n",
      "584   [https://storage.googleapis.com/kelas-app-test...    0  \n",
      "761   [https://storage.googleapis.com/kelas-app-test...    0  \n",
      "860   [https://storage.googleapis.com/kelas-app-test...    0  \n",
      "868   [https://storage.googleapis.com/kelas-app-test...    0  \n",
      "920   [https://storage.googleapis.com/kelas-app-test...    0  \n",
      "967   [https://storage.googleapis.com/kelas-app-test...    0  \n",
      "1145  [https://storage.googleapis.com/kelas-app-test...    0  \n",
      "1240  [https://storage.googleapis.com/kelas-app-test...    0  \n",
      "1273  [https://storage.googleapis.com/kelas-app-test...    0  \n",
      "1319  [https://storage.googleapis.com/kelas-app-test...    0  \n",
      "1344  [https://storage.googleapis.com/kelas-app-test...    0  \n"
     ]
    }
   ],
   "source": [
    "# Function to get recommendations for a specific user\n",
    "def recommend_products(user_id, model, interactions, user2user_encoded, product2product_encoded, productencoded2product, products, top_n=30):\n",
    "    # Check if user_id is in the encoding map\n",
    "    if user_id not in user2user_encoded:\n",
    "        print(f\"User ID {user_id} not found.\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    user_encoded = user2user_encoded[user_id]\n",
    "    # Get all encoded product IDs as a list of integers\n",
    "    product_ids = list(product2product_encoded.values())\n",
    "\n",
    "    # Create user-product array for prediction\n",
    "    # Ensure all entries are integers for the model input\n",
    "    user_product_array = np.array([[user_encoded] * len(product_ids), product_ids]).T.astype(int)\n",
    "\n",
    "    # Predict interaction values using the model\n",
    "    predictions = model.predict([user_product_array[:, 0], user_product_array[:, 1]])\n",
    "    predictions = predictions.flatten()\n",
    "\n",
    "    # Get top N product indices\n",
    "    top_indices = predictions.argsort()[-top_n:][::-1]\n",
    "    # Decode the top indices to product IDs\n",
    "    recommended_product_ids = [productencoded2product[x] for x in top_indices]\n",
    " \n",
    "    # Filter the products DataFrame to get recommended products using the correct column name\n",
    "    recommended_products = products[products['_id'].isin(recommended_product_ids)]\n",
    "    return recommended_products\n",
    "\n",
    "# Try the model with the specified user ID\n",
    "user_id = '6665eab57aa0dfec0ad43b2a'\n",
    "recommended_products = recommend_products(user_id, model, interactions, user2user_encoded, product2product_encoded, productencoded2product, products)\n",
    "print(f\"Recommended products for user {user_id}:\")\n",
    "print(recommended_products)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
