{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: tensorflow in c:\\users\\baihaqi\\appdata\\roaming\\python\\python311\\site-packages (2.12.0)\n",
      "Requirement already satisfied: python-dotenv in c:\\users\\baihaqi\\appdata\\roaming\\python\\python311\\site-packages (1.0.1)\n",
      "Requirement already satisfied: pandas in c:\\users\\baihaqi\\appdata\\roaming\\python\\python311\\site-packages (2.0.2)\n",
      "Requirement already satisfied: numpy in c:\\users\\baihaqi\\appdata\\roaming\\python\\python311\\site-packages (1.23.5)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\baihaqi\\appdata\\roaming\\python\\python311\\site-packages (1.5.0)\n",
      "Requirement already satisfied: tensorflow-intel==2.12.0 in c:\\users\\baihaqi\\appdata\\roaming\\python\\python311\\site-packages (from tensorflow) (2.12.0)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in c:\\users\\baihaqi\\appdata\\roaming\\python\\python311\\site-packages (from tensorflow-intel==2.12.0->tensorflow) (2.1.0)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in c:\\users\\baihaqi\\appdata\\roaming\\python\\python311\\site-packages (from tensorflow-intel==2.12.0->tensorflow) (1.6.3)\n",
      "Requirement already satisfied: flatbuffers>=2.0 in c:\\users\\baihaqi\\appdata\\roaming\\python\\python311\\site-packages (from tensorflow-intel==2.12.0->tensorflow) (24.3.25)\n",
      "Requirement already satisfied: gast<=0.4.0,>=0.2.1 in c:\\users\\baihaqi\\appdata\\roaming\\python\\python311\\site-packages (from tensorflow-intel==2.12.0->tensorflow) (0.4.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in c:\\users\\baihaqi\\appdata\\roaming\\python\\python311\\site-packages (from tensorflow-intel==2.12.0->tensorflow) (0.2.0)\n",
      "Requirement already satisfied: h5py>=2.9.0 in c:\\users\\baihaqi\\appdata\\roaming\\python\\python311\\site-packages (from tensorflow-intel==2.12.0->tensorflow) (3.11.0)\n",
      "Requirement already satisfied: jax>=0.3.15 in c:\\users\\baihaqi\\appdata\\roaming\\python\\python311\\site-packages (from tensorflow-intel==2.12.0->tensorflow) (0.4.29)\n",
      "Requirement already satisfied: libclang>=13.0.0 in c:\\users\\baihaqi\\appdata\\roaming\\python\\python311\\site-packages (from tensorflow-intel==2.12.0->tensorflow) (18.1.1)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in c:\\users\\baihaqi\\appdata\\roaming\\python\\python311\\site-packages (from tensorflow-intel==2.12.0->tensorflow) (3.3.0)\n",
      "Requirement already satisfied: packaging in c:\\program files\\python311\\lib\\site-packages (from tensorflow-intel==2.12.0->tensorflow) (23.2)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in c:\\users\\baihaqi\\appdata\\roaming\\python\\python311\\site-packages (from tensorflow-intel==2.12.0->tensorflow) (4.25.3)\n",
      "Requirement already satisfied: setuptools in c:\\program files\\python311\\lib\\site-packages (from tensorflow-intel==2.12.0->tensorflow) (65.5.0)\n",
      "Requirement already satisfied: six>=1.12.0 in c:\\users\\baihaqi\\appdata\\roaming\\python\\python311\\site-packages (from tensorflow-intel==2.12.0->tensorflow) (1.16.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in c:\\users\\baihaqi\\appdata\\roaming\\python\\python311\\site-packages (from tensorflow-intel==2.12.0->tensorflow) (2.4.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in c:\\users\\baihaqi\\appdata\\roaming\\python\\python311\\site-packages (from tensorflow-intel==2.12.0->tensorflow) (4.11.0)\n",
      "Requirement already satisfied: wrapt<1.15,>=1.11.0 in c:\\users\\baihaqi\\appdata\\roaming\\python\\python311\\site-packages (from tensorflow-intel==2.12.0->tensorflow) (1.14.1)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in c:\\users\\baihaqi\\appdata\\roaming\\python\\python311\\site-packages (from tensorflow-intel==2.12.0->tensorflow) (1.64.1)\n",
      "Requirement already satisfied: tensorboard<2.13,>=2.12 in c:\\users\\baihaqi\\appdata\\roaming\\python\\python311\\site-packages (from tensorflow-intel==2.12.0->tensorflow) (2.12.3)\n",
      "Requirement already satisfied: tensorflow-estimator<2.13,>=2.12.0 in c:\\users\\baihaqi\\appdata\\roaming\\python\\python311\\site-packages (from tensorflow-intel==2.12.0->tensorflow) (2.12.0)\n",
      "Requirement already satisfied: keras<2.13,>=2.12.0 in c:\\users\\baihaqi\\appdata\\roaming\\python\\python311\\site-packages (from tensorflow-intel==2.12.0->tensorflow) (2.12.0)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in c:\\users\\baihaqi\\appdata\\roaming\\python\\python311\\site-packages (from tensorflow-intel==2.12.0->tensorflow) (0.31.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\baihaqi\\appdata\\roaming\\python\\python311\\site-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\baihaqi\\appdata\\roaming\\python\\python311\\site-packages (from pandas) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in c:\\users\\baihaqi\\appdata\\roaming\\python\\python311\\site-packages (from pandas) (2024.1)\n",
      "Requirement already satisfied: scipy>=1.6.0 in c:\\users\\baihaqi\\appdata\\roaming\\python\\python311\\site-packages (from scikit-learn) (1.13.1)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\users\\baihaqi\\appdata\\roaming\\python\\python311\\site-packages (from scikit-learn) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\baihaqi\\appdata\\roaming\\python\\python311\\site-packages (from scikit-learn) (3.5.0)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in c:\\users\\baihaqi\\appdata\\roaming\\python\\python311\\site-packages (from astunparse>=1.6.0->tensorflow-intel==2.12.0->tensorflow) (0.43.0)\n",
      "Requirement already satisfied: ml-dtypes>=0.4.0 in c:\\users\\baihaqi\\appdata\\roaming\\python\\python311\\site-packages (from jax>=0.3.15->tensorflow-intel==2.12.0->tensorflow) (0.4.0)\n",
      "Requirement already satisfied: google-auth<3,>=1.6.3 in c:\\users\\baihaqi\\appdata\\roaming\\python\\python311\\site-packages (from tensorboard<2.13,>=2.12->tensorflow-intel==2.12.0->tensorflow) (2.30.0)\n",
      "Requirement already satisfied: google-auth-oauthlib<1.1,>=0.5 in c:\\users\\baihaqi\\appdata\\roaming\\python\\python311\\site-packages (from tensorboard<2.13,>=2.12->tensorflow-intel==2.12.0->tensorflow) (1.0.0)\n",
      "Requirement already satisfied: markdown>=2.6.8 in c:\\users\\baihaqi\\appdata\\roaming\\python\\python311\\site-packages (from tensorboard<2.13,>=2.12->tensorflow-intel==2.12.0->tensorflow) (3.6)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in c:\\users\\baihaqi\\appdata\\roaming\\python\\python311\\site-packages (from tensorboard<2.13,>=2.12->tensorflow-intel==2.12.0->tensorflow) (2.31.0)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in c:\\users\\baihaqi\\appdata\\roaming\\python\\python311\\site-packages (from tensorboard<2.13,>=2.12->tensorflow-intel==2.12.0->tensorflow) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in c:\\users\\baihaqi\\appdata\\roaming\\python\\python311\\site-packages (from tensorboard<2.13,>=2.12->tensorflow-intel==2.12.0->tensorflow) (3.0.3)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in c:\\users\\baihaqi\\appdata\\roaming\\python\\python311\\site-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow-intel==2.12.0->tensorflow) (5.3.3)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in c:\\users\\baihaqi\\appdata\\roaming\\python\\python311\\site-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow-intel==2.12.0->tensorflow) (0.4.0)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in c:\\users\\baihaqi\\appdata\\roaming\\python\\python311\\site-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow-intel==2.12.0->tensorflow) (4.9)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in c:\\users\\baihaqi\\appdata\\roaming\\python\\python311\\site-packages (from google-auth-oauthlib<1.1,>=0.5->tensorboard<2.13,>=2.12->tensorflow-intel==2.12.0->tensorflow) (1.3.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\baihaqi\\appdata\\roaming\\python\\python311\\site-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow-intel==2.12.0->tensorflow) (3.3.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\baihaqi\\appdata\\roaming\\python\\python311\\site-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow-intel==2.12.0->tensorflow) (2.9)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\baihaqi\\appdata\\roaming\\python\\python311\\site-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow-intel==2.12.0->tensorflow) (1.25.8)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\baihaqi\\appdata\\roaming\\python\\python311\\site-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow-intel==2.12.0->tensorflow) (2020.4.5.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in c:\\users\\baihaqi\\appdata\\roaming\\python\\python311\\site-packages (from werkzeug>=1.0.1->tensorboard<2.13,>=2.12->tensorflow-intel==2.12.0->tensorflow) (2.1.3)\n",
      "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in c:\\users\\baihaqi\\appdata\\roaming\\python\\python311\\site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow-intel==2.12.0->tensorflow) (0.6.0)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in c:\\users\\baihaqi\\appdata\\roaming\\python\\python311\\site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<1.1,>=0.5->tensorboard<2.13,>=2.12->tensorflow-intel==2.12.0->tensorflow) (3.2.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install tensorflow python-dotenv pandas numpy scikit-learn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Necessary Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Embedding, Flatten, Dot, Dense\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from sklearn.model_selection import train_test_split\n",
    "from dotenv import load_dotenv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data successfully fetched from http://161.97.109.65:3000/api/users\n",
      "                        _id       firstname       lastname         username  \\\n",
      "0  6665e9847aa0dfec0ad43b26         Machine       Learning  machinelearning   \n",
      "1  6665eab57aa0dfec0ad43b2a  DummyFirstname  DummyLastname       dummydata1   \n",
      "2  6665eac87aa0dfec0ad43b2d  DummyFirstname  DummyLastname       dummydata2   \n",
      "3  6665eacc7aa0dfec0ad43b30  DummyFirstname  DummyLastname       dummydata3   \n",
      "4  6665eacf7aa0dfec0ad43b33  DummyFirstname  DummyLastname       dummydata4   \n",
      "\n",
      "                  email         phone  \\\n",
      "0          ml@admin.com    7777777777   \n",
      "1  dummydata1@admin.com  777777770001   \n",
      "2  dummydata2@admin.com  777777770002   \n",
      "3  dummydata3@admin.com  777777770003   \n",
      "4  dummydata4@admin.com  777777770004   \n",
      "\n",
      "                                            password           address  \\\n",
      "0  $2a$10$oNSoSQcmxvHAefk5dKx0UuJw8oSdGeCumA.ZqIN...  Bangkit Capstone   \n",
      "1  $2a$10$ihldsbescWBR9v94/sRhReBpX8mZMGrRpwkUohU...   Dummy Address 1   \n",
      "2  $2a$10$IipmxQztB7MnyyTVUka6n.IK9C/wdqcEf8SXZjD...   Dummy Address 2   \n",
      "3  $2a$10$mQEfWkNV4c6.E6glZRmlyuNzrAVKctoLLXMv2pK...   Dummy Address 3   \n",
      "4  $2a$10$GYFAH4GoxInzAr8WkAeUEuLTYD5ZLjFumTyR2ZP...   Dummy Address 4   \n",
      "\n",
      "                        avatar   role                 createdat  \\\n",
      "0  /uploads/default_avatar.png  buyer  2024-06-09T17:42:28.211Z   \n",
      "1  /uploads/default_avatar.png  buyer  2024-06-09T17:47:33.477Z   \n",
      "2  /uploads/default_avatar.png  buyer  2024-06-09T17:47:52.685Z   \n",
      "3  /uploads/default_avatar.png  buyer  2024-06-09T17:47:56.118Z   \n",
      "4  /uploads/default_avatar.png  buyer  2024-06-09T17:47:59.747Z   \n",
      "\n",
      "                  updatedat ratings  __v  \n",
      "0  2024-06-09T17:42:28.211Z      []    0  \n",
      "1  2024-06-09T17:47:33.477Z      []    0  \n",
      "2  2024-06-09T17:47:52.685Z      []    0  \n",
      "3  2024-06-09T17:47:56.118Z      []    0  \n",
      "4  2024-06-09T17:47:59.747Z      []    0  \n",
      "Data successfully fetched from http://161.97.109.65:3000/api/products\n",
      "                        _id category     price  \\\n",
      "0  6667ef73b3e75416b2fa7e33     Meja  155000.0   \n",
      "1  6667ef73b3e75416b2fa7e34     Meja  124000.0   \n",
      "2  6667ef73b3e75416b2fa7e35     Meja  107000.0   \n",
      "3  6667ef73b3e75416b2fa7e36     Meja   99500.0   \n",
      "4  6667ef73b3e75416b2fa7e37     Meja  446000.0   \n",
      "\n",
      "                                                name  \\\n",
      "0  Damaindah Meja Belajar Kayu Set Kursi / Meja B...   \n",
      "1  Homedoki Meja / Meja Makan / Meja Komputer / M...   \n",
      "2  Sakula Meja kantor meja kerja Meja Komputer Pe...   \n",
      "3  Meja Portable Stand Laptop Meja Laptop Standin...   \n",
      "4  PiPi Furniture Meja Gaming / Meja komputer / M...   \n",
      "\n",
      "                   sellerId  \\\n",
      "0  6665e9847aa0dfec0ad43b26   \n",
      "1  6665e9847aa0dfec0ad43b26   \n",
      "2  6665e9847aa0dfec0ad43b26   \n",
      "3  6665e9847aa0dfec0ad43b26   \n",
      "4  6665e9847aa0dfec0ad43b26   \n",
      "\n",
      "                                        productImage  __v  \n",
      "0  [https://storage.googleapis.com/kelas-app-test...    0  \n",
      "1  [https://storage.googleapis.com/kelas-app-test...    0  \n",
      "2  [https://storage.googleapis.com/kelas-app-test...    0  \n",
      "3  [https://storage.googleapis.com/kelas-app-test...    0  \n",
      "4  [https://storage.googleapis.com/kelas-app-test...    0  \n",
      "Data successfully fetched from http://161.97.109.65:3000/api/interactions\n",
      "                                        interactions\n",
      "0  {'_id': '6667f373b3e75416b2fa83a7', 'userId': ...\n",
      "1  {'_id': '6667f373b3e75416b2fa83a8', 'userId': ...\n",
      "2  {'_id': '6667f373b3e75416b2fa83a9', 'userId': ...\n",
      "3  {'_id': '6667f373b3e75416b2fa83aa', 'userId': ...\n",
      "4  {'_id': '6667f373b3e75416b2fa83ab', 'userId': ...\n",
      "All data fetched successfully.\n"
     ]
    }
   ],
   "source": [
    "load_dotenv()  # Load environment variables from .env file\n",
    "jwt_token = os.getenv('JWT_TOKEN')\n",
    "\n",
    "headers = {'Authorization': f'Bearer {jwt_token}'}\n",
    "\n",
    "api_urls = {\n",
    "    'interactions': 'http://161.97.109.65:3000/api/interactions',\n",
    "    'users': 'http://161.97.109.65:3000/api/users',\n",
    "    'products': 'http://161.97.109.65:3000/api/products'\n",
    "}\n",
    "\n",
    "def fetch_data(url, headers):\n",
    "    try:\n",
    "        response = requests.get(url, headers=headers)\n",
    "        response.raise_for_status()  # Raises an HTTPError for bad requests\n",
    "        data = pd.DataFrame(response.json())\n",
    "        print(f\"Data successfully fetched from {url}\")\n",
    "        print(data.head())  # Display the first few rows of the DataFrame\n",
    "        return data\n",
    "    except requests.RequestException as e:\n",
    "        print(f'Failed to fetch data from {url}: {str(e)}')\n",
    "        return pd.DataFrame()\n",
    "\n",
    "# Fetch data from APIs\n",
    "users = fetch_data(api_urls['users'], headers)\n",
    "products = fetch_data(api_urls['products'], headers)\n",
    "interactions = fetch_data(api_urls['interactions'], headers)\n",
    "\n",
    "# Check if data was fetched successfully\n",
    "if not users.empty and not products.empty and not interactions.empty:\n",
    "    print(\"All data fetched successfully.\")\n",
    "else:\n",
    "    print(\"Data fetching failed, check errors and retry.\")\n",
    "    # Optionally, add logic to halt further processing if data is crucial\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's assume 'interactions' is a DataFrame with a column containing dictionaries\n",
    "# First, ensure that the 'interactions' column is appropriately normalized\n",
    "if 'interactions' in interactions.columns:\n",
    "    interactions_expanded = pd.json_normalize(interactions['interactions'])\n",
    "else:\n",
    "    interactions_expanded = pd.json_normalize(interactions.iloc[:, 0])  # If 'interactions' is the name of DataFrame and not a column\n",
    "\n",
    "# Assuming the JSON data has keys 'userId', 'productId', and 'interactionValue'\n",
    "interactions_expanded['user_id'] = interactions_expanded['userId']\n",
    "interactions_expanded['product_id'] = interactions_expanded['productId']\n",
    "interactions_expanded['interaction_value'] = interactions_expanded['interactionValue']\n",
    "\n",
    "# Encode user_id and product_id\n",
    "user_ids = interactions_expanded['user_id'].unique().tolist()\n",
    "product_ids = interactions_expanded['product_id'].unique().tolist()\n",
    "\n",
    "user2user_encoded = {x: i for i, x in enumerate(user_ids)}\n",
    "product2product_encoded = {x: i for i, x in enumerate(product_ids)}\n",
    "userencoded2user = {i: x for i, x in enumerate(user_ids)}\n",
    "productencoded2product = {i: x for i, x in enumerate(product_ids)}\n",
    "\n",
    "interactions_expanded['user'] = interactions_expanded['user_id'].map(user2user_encoded)\n",
    "interactions_expanded['product'] = interactions_expanded['product_id'].map(product2product_encoded)\n",
    "\n",
    "# Split the data\n",
    "train, test = train_test_split(interactions_expanded, test_size=0.2, random_state=42)\n",
    "\n",
    "# Convert data into required format\n",
    "x_train = [train['user'].values, train['product'].values]\n",
    "y_train = train['interaction_value'].values\n",
    "x_test = [test['user'].values, test['product'].values]\n",
    "y_test = test['interaction_value'].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_input = Input(shape=(1,))\n",
    "user_embedding = Embedding(len(user2user_encoded), 50)(user_input)  # Removed input_length\n",
    "user_vec = Flatten()(user_embedding)\n",
    "\n",
    "product_input = Input(shape=(1,))\n",
    "product_embedding = Embedding(len(product2product_encoded), 50)(product_input)  # Removed input_length\n",
    "product_vec = Flatten()(product_embedding)\n",
    "\n",
    "dot_product = Dot(axes=1)([user_vec, product_vec])\n",
    "model = Model(inputs=[user_input, product_input], outputs=dot_product)\n",
    "model.compile(optimizer=Adam(), loss='mean_squared_error')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "40/40 [==============================] - 1s 4ms/step - loss: 4.6993 - val_loss: 4.5370\n",
      "Epoch 2/20\n",
      "40/40 [==============================] - 0s 1ms/step - loss: 4.6662 - val_loss: 4.5350\n",
      "Epoch 3/20\n",
      "40/40 [==============================] - 0s 1ms/step - loss: 4.6126 - val_loss: 4.5303\n",
      "Epoch 4/20\n",
      "40/40 [==============================] - 0s 2ms/step - loss: 4.5090 - val_loss: 4.5190\n",
      "Epoch 5/20\n",
      "40/40 [==============================] - 0s 2ms/step - loss: 4.3268 - val_loss: 4.4998\n",
      "Epoch 6/20\n",
      "40/40 [==============================] - 0s 2ms/step - loss: 4.0444 - val_loss: 4.4685\n",
      "Epoch 7/20\n",
      "40/40 [==============================] - 0s 1ms/step - loss: 3.6602 - val_loss: 4.4246\n",
      "Epoch 8/20\n",
      "40/40 [==============================] - 0s 1ms/step - loss: 3.1926 - val_loss: 4.3748\n",
      "Epoch 9/20\n",
      "40/40 [==============================] - 0s 1ms/step - loss: 2.6807 - val_loss: 4.3169\n",
      "Epoch 10/20\n",
      "40/40 [==============================] - 0s 2ms/step - loss: 2.1666 - val_loss: 4.2558\n",
      "Epoch 11/20\n",
      "40/40 [==============================] - 0s 2ms/step - loss: 1.6913 - val_loss: 4.1986\n",
      "Epoch 12/20\n",
      "40/40 [==============================] - 0s 2ms/step - loss: 1.2776 - val_loss: 4.1469\n",
      "Epoch 13/20\n",
      "40/40 [==============================] - 0s 1ms/step - loss: 0.9386 - val_loss: 4.1058\n",
      "Epoch 14/20\n",
      "40/40 [==============================] - 0s 1ms/step - loss: 0.6731 - val_loss: 4.0711\n",
      "Epoch 15/20\n",
      "40/40 [==============================] - 0s 1ms/step - loss: 0.4737 - val_loss: 4.0441\n",
      "Epoch 16/20\n",
      "40/40 [==============================] - 0s 1ms/step - loss: 0.3320 - val_loss: 4.0235\n",
      "Epoch 17/20\n",
      "40/40 [==============================] - 0s 1ms/step - loss: 0.2327 - val_loss: 4.0089\n",
      "Epoch 18/20\n",
      "40/40 [==============================] - 0s 1ms/step - loss: 0.1644 - val_loss: 3.9972\n",
      "Epoch 19/20\n",
      "40/40 [==============================] - 0s 1ms/step - loss: 0.1178 - val_loss: 3.9898\n",
      "Epoch 20/20\n",
      "40/40 [==============================] - 0s 1ms/step - loss: 0.0863 - val_loss: 3.9852\n",
      "Epoch 1/20\n",
      " 1/40 [..............................] - ETA: 0s - loss: 0.0638\n",
      "Epoch 1: val_loss improved from inf to 3.98233, saving model to ..\\model\\best_model.keras\n",
      "40/40 [==============================] - 0s 3ms/step - loss: 0.0644 - val_loss: 3.9823\n",
      "Epoch 2/20\n",
      " 1/40 [..............................] - ETA: 0s - loss: 0.0240\n",
      "Epoch 2: val_loss improved from 3.98233 to 3.97988, saving model to ..\\model\\best_model.keras\n",
      "40/40 [==============================] - 0s 2ms/step - loss: 0.0497 - val_loss: 3.9799\n",
      "Epoch 3/20\n",
      " 1/40 [..............................] - ETA: 0s - loss: 0.0786\n",
      "Epoch 3: val_loss improved from 3.97988 to 3.97957, saving model to ..\\model\\best_model.keras\n",
      "40/40 [==============================] - 0s 2ms/step - loss: 0.0400 - val_loss: 3.9796\n",
      "Epoch 4/20\n",
      " 1/40 [..............................] - ETA: 0s - loss: 0.0355\n",
      "Epoch 4: val_loss improved from 3.97957 to 3.97810, saving model to ..\\model\\best_model.keras\n",
      "40/40 [==============================] - 0s 2ms/step - loss: 0.0324 - val_loss: 3.9781\n",
      "Epoch 5/20\n",
      " 1/40 [..............................] - ETA: 0s - loss: 0.0600\n",
      "Epoch 5: val_loss improved from 3.97810 to 3.97780, saving model to ..\\model\\best_model.keras\n",
      "40/40 [==============================] - 0s 2ms/step - loss: 0.0277 - val_loss: 3.9778\n",
      "Epoch 6/20\n",
      " 1/40 [..............................] - ETA: 0s - loss: 0.0210\n",
      "Epoch 6: val_loss improved from 3.97780 to 3.97764, saving model to ..\\model\\best_model.keras\n",
      "40/40 [==============================] - 0s 2ms/step - loss: 0.0239 - val_loss: 3.9776\n",
      "Epoch 7/20\n",
      " 1/40 [..............................] - ETA: 0s - loss: 0.0300\n",
      "Epoch 7: val_loss improved from 3.97764 to 3.97756, saving model to ..\\model\\best_model.keras\n",
      "40/40 [==============================] - 0s 2ms/step - loss: 0.0216 - val_loss: 3.9776\n",
      "Epoch 8/20\n",
      " 1/40 [..............................] - ETA: 0s - loss: 0.0047\n",
      "Epoch 8: val_loss improved from 3.97756 to 3.97742, saving model to ..\\model\\best_model.keras\n",
      "40/40 [==============================] - 0s 2ms/step - loss: 0.0200 - val_loss: 3.9774\n",
      "Epoch 9/20\n",
      " 1/40 [..............................] - ETA: 0s - loss: 0.0122\n",
      "Epoch 9: val_loss improved from 3.97742 to 3.97722, saving model to ..\\model\\best_model.keras\n",
      "40/40 [==============================] - 0s 2ms/step - loss: 0.0183 - val_loss: 3.9772\n",
      "Epoch 10/20\n",
      " 1/40 [..............................] - ETA: 0s - loss: 0.0083\n",
      "Epoch 10: val_loss improved from 3.97722 to 3.97700, saving model to ..\\model\\best_model.keras\n",
      "40/40 [==============================] - 0s 2ms/step - loss: 0.0173 - val_loss: 3.9770\n",
      "Epoch 11/20\n",
      " 1/40 [..............................] - ETA: 0s - loss: 6.7149e-04\n",
      "Epoch 11: val_loss did not improve from 3.97700\n",
      "40/40 [==============================] - 0s 1ms/step - loss: 0.0164 - val_loss: 3.9773\n",
      "Epoch 12/20\n",
      " 1/40 [..............................] - ETA: 0s - loss: 0.0297\n",
      "Epoch 12: val_loss improved from 3.97700 to 3.97697, saving model to ..\\model\\best_model.keras\n",
      "40/40 [==============================] - 0s 2ms/step - loss: 0.0159 - val_loss: 3.9770\n",
      "Epoch 13/20\n",
      " 1/40 [..............................] - ETA: 0s - loss: 0.0259\n",
      "Epoch 13: val_loss did not improve from 3.97697\n",
      "40/40 [==============================] - 0s 1ms/step - loss: 0.0155 - val_loss: 3.9772\n",
      "Epoch 14/20\n",
      " 1/40 [..............................] - ETA: 0s - loss: 0.0611\n",
      "Epoch 14: val_loss did not improve from 3.97697\n",
      "40/40 [==============================] - 0s 1ms/step - loss: 0.0156 - val_loss: 3.9775\n",
      "Epoch 15/20\n",
      " 1/40 [..............................] - ETA: 0s - loss: 7.1429e-05\n",
      "Epoch 15: val_loss did not improve from 3.97697\n",
      "40/40 [==============================] - 0s 1ms/step - loss: 0.0151 - val_loss: 3.9773\n",
      "Epoch 16/20\n",
      " 1/40 [..............................] - ETA: 0s - loss: 1.4834e-04\n",
      "Epoch 16: val_loss did not improve from 3.97697\n",
      "40/40 [==============================] - 0s 1ms/step - loss: 0.0149 - val_loss: 3.9772\n",
      "Epoch 17/20\n",
      " 1/40 [..............................] - ETA: 0s - loss: 3.7395e-04Restoring model weights from the end of the best epoch: 12.\n",
      "\n",
      "Epoch 17: val_loss did not improve from 3.97697\n",
      "40/40 [==============================] - 0s 1ms/step - loss: 0.0148 - val_loss: 3.9776\n",
      "Epoch 17: early stopping\n"
     ]
    }
   ],
   "source": [
    "# Assuming 'train' and 'test' datasets are already split and preprocessed\n",
    "x_train = [np.array(train['user']), np.array(train['product'])]\n",
    "y_train = np.array(train['interaction_value'])\n",
    "\n",
    "# Fit the model\n",
    "history = model.fit(\n",
    "    x_train, \n",
    "    y_train, \n",
    "    epochs=20, \n",
    "    verbose=1, \n",
    "    validation_split=0.2\n",
    ")\n",
    "\n",
    "# Optionally, you can add callbacks, for example to save the best model or early stopping\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "\n",
    "# Define the path to the model directory\n",
    "model_dir = os.path.join('..', 'model', 'collaborative_model.keras')\n",
    "\n",
    "# Setup callbacks\n",
    "callbacks = [\n",
    "    EarlyStopping(monitor='val_loss', patience=5, verbose=1, restore_best_weights=True),\n",
    "    ModelCheckpoint(model_dir, monitor='val_loss', save_best_only=True, verbose=1)\n",
    "]\n",
    "\n",
    "# Fit the model with callbacks\n",
    "history = model.fit(\n",
    "    x_train, \n",
    "    y_train, \n",
    "    epochs=20, \n",
    "    verbose=1, \n",
    "    validation_split=0.2,\n",
    "    callbacks=callbacks\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Try Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User ID 666545a42b9108ea2b463d87 not found.\n",
      "Recommended products for user 666545a42b9108ea2b463d87:\n",
      "Empty DataFrame\n",
      "Columns: []\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "# Function to get recommendations for a specific user\n",
    "def recommend_products(user_id, model, interactions, user2user_encoded, product2product_encoded, productencoded2product, products, top_n=30):\n",
    "    # Check if user_id is in the encoding map\n",
    "    if user_id not in user2user_encoded:\n",
    "        print(f\"User ID {user_id} not found.\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    user_encoded = user2user_encoded[user_id]\n",
    "    # Get all encoded product IDs as a list of integers\n",
    "    product_ids = list(product2product_encoded.values())\n",
    "\n",
    "    # Create user-product array for prediction\n",
    "    # Ensure all entries are integers for the model input\n",
    "    user_product_array = np.array([[user_encoded] * len(product_ids), product_ids]).T.astype(int)\n",
    "\n",
    "    # Predict interaction values using the model\n",
    "    predictions = model.predict([user_product_array[:, 0], user_product_array[:, 1]])\n",
    "    predictions = predictions.flatten()\n",
    "\n",
    "    # Get top N product indices\n",
    "    top_indices = predictions.argsort()[-top_n:][::-1]\n",
    "    # Decode the top indices to product IDs\n",
    "    recommended_product_ids = [productencoded2product[x] for x in top_indices]\n",
    "\n",
    "    # Filter the products DataFrame to get recommended products using the correct column name\n",
    "    recommended_products = products[products['_id'].isin(recommended_product_ids)]\n",
    "    return recommended_products\n",
    "\n",
    "# Try the model with the specified user ID\n",
    "user_id = '666545a42b9108ea2b463d87'\n",
    "recommended_products = recommend_products(user_id, model, interactions, user2user_encoded, product2product_encoded, productencoded2product, products)\n",
    "print(f\"Recommended products for user {user_id}:\")\n",
    "print(recommended_products)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
