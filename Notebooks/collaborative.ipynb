{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tensorflow in c:\\users\\user\\miniconda3\\lib\\site-packages (2.16.1)\n",
      "Requirement already satisfied: python-dotenv in c:\\users\\user\\miniconda3\\lib\\site-packages (1.0.1)\n",
      "Requirement already satisfied: pandas in c:\\users\\user\\miniconda3\\lib\\site-packages (2.2.2)\n",
      "Requirement already satisfied: numpy in c:\\users\\user\\miniconda3\\lib\\site-packages (1.26.4)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\user\\miniconda3\\lib\\site-packages (1.5.0)\n",
      "Requirement already satisfied: tensorflow-intel==2.16.1 in c:\\users\\user\\miniconda3\\lib\\site-packages (from tensorflow) (2.16.1)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in c:\\users\\user\\miniconda3\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (2.1.0)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in c:\\users\\user\\miniconda3\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (1.6.3)\n",
      "Requirement already satisfied: flatbuffers>=23.5.26 in c:\\users\\user\\miniconda3\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (24.3.25)\n",
      "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in c:\\users\\user\\miniconda3\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (0.5.4)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in c:\\users\\user\\miniconda3\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (0.2.0)\n",
      "Requirement already satisfied: h5py>=3.10.0 in c:\\users\\user\\miniconda3\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (3.11.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in c:\\users\\user\\miniconda3\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (18.1.1)\n",
      "Requirement already satisfied: ml-dtypes~=0.3.1 in c:\\users\\user\\miniconda3\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (0.3.2)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in c:\\users\\user\\miniconda3\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (3.3.0)\n",
      "Requirement already satisfied: packaging in c:\\users\\user\\miniconda3\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (23.1)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in c:\\users\\user\\miniconda3\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (4.25.3)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in c:\\users\\user\\miniconda3\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (2.31.0)\n",
      "Requirement already satisfied: setuptools in c:\\users\\user\\miniconda3\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (68.0.0)\n",
      "Requirement already satisfied: six>=1.12.0 in c:\\users\\user\\miniconda3\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (1.16.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in c:\\users\\user\\miniconda3\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (2.4.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in c:\\users\\user\\miniconda3\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (4.9.0)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in c:\\users\\user\\miniconda3\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (1.16.0)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in c:\\users\\user\\miniconda3\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (1.64.1)\n",
      "Requirement already satisfied: tensorboard<2.17,>=2.16 in c:\\users\\user\\miniconda3\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (2.16.2)\n",
      "Requirement already satisfied: keras>=3.0.0 in c:\\users\\user\\miniconda3\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (3.3.3)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in c:\\users\\user\\miniconda3\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (0.31.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\user\\miniconda3\\lib\\site-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\user\\miniconda3\\lib\\site-packages (from pandas) (2023.3.post1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\user\\miniconda3\\lib\\site-packages (from pandas) (2023.3)\n",
      "Requirement already satisfied: scipy>=1.6.0 in c:\\users\\user\\miniconda3\\lib\\site-packages (from scikit-learn) (1.11.4)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\users\\user\\miniconda3\\lib\\site-packages (from scikit-learn) (1.2.0)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\user\\miniconda3\\lib\\site-packages (from scikit-learn) (3.5.0)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in c:\\users\\user\\miniconda3\\lib\\site-packages (from astunparse>=1.6.0->tensorflow-intel==2.16.1->tensorflow) (0.41.2)\n",
      "Requirement already satisfied: rich in c:\\users\\user\\miniconda3\\lib\\site-packages (from keras>=3.0.0->tensorflow-intel==2.16.1->tensorflow) (13.7.1)\n",
      "Requirement already satisfied: namex in c:\\users\\user\\miniconda3\\lib\\site-packages (from keras>=3.0.0->tensorflow-intel==2.16.1->tensorflow) (0.0.8)\n",
      "Requirement already satisfied: optree in c:\\users\\user\\miniconda3\\lib\\site-packages (from keras>=3.0.0->tensorflow-intel==2.16.1->tensorflow) (0.11.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\user\\miniconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow-intel==2.16.1->tensorflow) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\user\\miniconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow-intel==2.16.1->tensorflow) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\user\\miniconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow-intel==2.16.1->tensorflow) (1.26.18)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\user\\miniconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow-intel==2.16.1->tensorflow) (2024.2.2)\n",
      "Requirement already satisfied: markdown>=2.6.8 in c:\\users\\user\\miniconda3\\lib\\site-packages (from tensorboard<2.17,>=2.16->tensorflow-intel==2.16.1->tensorflow) (3.6)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in c:\\users\\user\\miniconda3\\lib\\site-packages (from tensorboard<2.17,>=2.16->tensorflow-intel==2.16.1->tensorflow) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in c:\\users\\user\\miniconda3\\lib\\site-packages (from tensorboard<2.17,>=2.16->tensorflow-intel==2.16.1->tensorflow) (3.0.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in c:\\users\\user\\miniconda3\\lib\\site-packages (from werkzeug>=1.0.1->tensorboard<2.17,>=2.16->tensorflow-intel==2.16.1->tensorflow) (2.1.3)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in c:\\users\\user\\miniconda3\\lib\\site-packages (from rich->keras>=3.0.0->tensorflow-intel==2.16.1->tensorflow) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\users\\user\\miniconda3\\lib\\site-packages (from rich->keras>=3.0.0->tensorflow-intel==2.16.1->tensorflow) (2.15.1)\n",
      "Requirement already satisfied: mdurl~=0.1 in c:\\users\\user\\miniconda3\\lib\\site-packages (from markdown-it-py>=2.2.0->rich->keras>=3.0.0->tensorflow-intel==2.16.1->tensorflow) (0.1.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install tensorflow python-dotenv pandas numpy scikit-learn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Necessary Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Embedding, Flatten, Dot, Dense\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from sklearn.model_selection import train_test_split\n",
    "from dotenv import load_dotenv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data successfully fetched from http://161.97.109.65:3000/api/users\n",
      "                        _id       firstname       lastname         username  \\\n",
      "0  6665e9847aa0dfec0ad43b26         Machine       Learning  machinelearning   \n",
      "1  6665eab57aa0dfec0ad43b2a  DummyFirstname  DummyLastname       dummydata1   \n",
      "2  6665eac87aa0dfec0ad43b2d  DummyFirstname  DummyLastname       dummydata2   \n",
      "3  6665eacc7aa0dfec0ad43b30  DummyFirstname  DummyLastname       dummydata3   \n",
      "4  6665eacf7aa0dfec0ad43b33  DummyFirstname  DummyLastname       dummydata4   \n",
      "\n",
      "                  email         phone  \\\n",
      "0          ml@admin.com    7777777777   \n",
      "1  dummydata1@admin.com  777777770001   \n",
      "2  dummydata2@admin.com  777777770002   \n",
      "3  dummydata3@admin.com  777777770003   \n",
      "4  dummydata4@admin.com  777777770004   \n",
      "\n",
      "                                            password           address  \\\n",
      "0  $2a$10$oNSoSQcmxvHAefk5dKx0UuJw8oSdGeCumA.ZqIN...  Bangkit Capstone   \n",
      "1  $2a$10$ihldsbescWBR9v94/sRhReBpX8mZMGrRpwkUohU...   Dummy Address 1   \n",
      "2  $2a$10$IipmxQztB7MnyyTVUka6n.IK9C/wdqcEf8SXZjD...   Dummy Address 2   \n",
      "3  $2a$10$mQEfWkNV4c6.E6glZRmlyuNzrAVKctoLLXMv2pK...   Dummy Address 3   \n",
      "4  $2a$10$GYFAH4GoxInzAr8WkAeUEuLTYD5ZLjFumTyR2ZP...   Dummy Address 4   \n",
      "\n",
      "                        avatar   role                 createdat  \\\n",
      "0  /uploads/default_avatar.png  buyer  2024-06-09T17:42:28.211Z   \n",
      "1  /uploads/default_avatar.png  buyer  2024-06-09T17:47:33.477Z   \n",
      "2  /uploads/default_avatar.png  buyer  2024-06-09T17:47:52.685Z   \n",
      "3  /uploads/default_avatar.png  buyer  2024-06-09T17:47:56.118Z   \n",
      "4  /uploads/default_avatar.png  buyer  2024-06-09T17:47:59.747Z   \n",
      "\n",
      "                  updatedat ratings  __v  \n",
      "0  2024-06-09T17:42:28.211Z      []    0  \n",
      "1  2024-06-09T17:47:33.477Z      []    0  \n",
      "2  2024-06-09T17:47:52.685Z      []    0  \n",
      "3  2024-06-09T17:47:56.118Z      []    0  \n",
      "4  2024-06-09T17:47:59.747Z      []    0  \n",
      "Data successfully fetched from http://161.97.109.65:3000/api/products\n",
      "                        _id              name           category  \\\n",
      "0  6665ecb77aa0dfec0ad43b69         Textbooks              Books   \n",
      "1  6665ecf07aa0dfec0ad43b6c    School Uniform           Clothing   \n",
      "2  6665ed0f7aa0dfec0ad43b6f          Backpack        Accessories   \n",
      "3  6665ed747aa0dfec0ad43b73  Sports Equipment  Sports & Outdoors   \n",
      "4  6665ed8e7aa0dfec0ad43b76            Laptop        Electronics   \n",
      "\n",
      "                                         description   price  \\\n",
      "0        Gently used textbooks for various subjects.   15.99   \n",
      "1  Pre-owned boarding school uniform in good cond...   29.99   \n",
      "2  Sturdy backpack suitable for boarding school e...   24.99   \n",
      "3   Used sports gear for extracurricular activities.   49.99   \n",
      "4      Refurbished laptop ideal for school projects.  199.99   \n",
      "\n",
      "                   sellerId  \\\n",
      "0  6665e9847aa0dfec0ad43b26   \n",
      "1  6665e9847aa0dfec0ad43b26   \n",
      "2  6665e9847aa0dfec0ad43b26   \n",
      "3  6665e9847aa0dfec0ad43b26   \n",
      "4  6665e9847aa0dfec0ad43b26   \n",
      "\n",
      "                                        productImage  __v  \n",
      "0  [https://storage.googleapis.com/kelas-app-test...    0  \n",
      "1  [https://storage.googleapis.com/kelas-app-test...    0  \n",
      "2  [https://storage.googleapis.com/kelas-app-test...    0  \n",
      "3  [https://storage.googleapis.com/kelas-app-test...    0  \n",
      "4  [https://storage.googleapis.com/kelas-app-test...    0  \n",
      "Data successfully fetched from http://161.97.109.65:3000/api/interactions\n",
      "                                        interactions\n",
      "0  {'_id': '66660baf89ae27a271fbf589', 'userId': ...\n",
      "1  {'_id': '66660bc289ae27a271fbf590', 'userId': ...\n",
      "2  {'_id': '66660bd689ae27a271fbf596', 'userId': ...\n",
      "3  {'_id': '66660bf289ae27a271fbf59a', 'userId': ...\n",
      "4  {'_id': '66660c7789ae27a271fbf5a2', 'userId': ...\n",
      "All data fetched successfully.\n"
     ]
    }
   ],
   "source": [
    "load_dotenv()  # Load environment variables from .env file\n",
    "jwt_token = os.getenv('JWT_TOKEN')\n",
    "\n",
    "headers = {'Authorization': f'Bearer {jwt_token}'}\n",
    "\n",
    "api_urls = {\n",
    "    'interactions': 'http://161.97.109.65:3000/api/interactions',\n",
    "    'users': 'http://161.97.109.65:3000/api/users',\n",
    "    'products': 'http://161.97.109.65:3000/api/products'\n",
    "}\n",
    "\n",
    "def fetch_data(url, headers):\n",
    "    try:\n",
    "        response = requests.get(url, headers=headers)\n",
    "        response.raise_for_status()  # Raises an HTTPError for bad requests\n",
    "        data = pd.DataFrame(response.json())\n",
    "        print(f\"Data successfully fetched from {url}\")\n",
    "        print(data.head())  # Display the first few rows of the DataFrame\n",
    "        return data\n",
    "    except requests.RequestException as e:\n",
    "        print(f'Failed to fetch data from {url}: {str(e)}')\n",
    "        return pd.DataFrame()\n",
    "\n",
    "# Fetch data from APIs\n",
    "users = fetch_data(api_urls['users'], headers)\n",
    "products = fetch_data(api_urls['products'], headers)\n",
    "interactions = fetch_data(api_urls['interactions'], headers)\n",
    "\n",
    "# Check if data was fetched successfully\n",
    "if not users.empty and not products.empty and not interactions.empty:\n",
    "    print(\"All data fetched successfully.\")\n",
    "else:\n",
    "    print(\"Data fetching failed, check errors and retry.\")\n",
    "    # Optionally, add logic to halt further processing if data is crucial\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's assume 'interactions' is a DataFrame with a column containing dictionaries\n",
    "# First, ensure that the 'interactions' column is appropriately normalized\n",
    "if 'interactions' in interactions.columns:\n",
    "    interactions_expanded = pd.json_normalize(interactions['interactions'])\n",
    "else:\n",
    "    interactions_expanded = pd.json_normalize(interactions.iloc[:, 0])  # If 'interactions' is the name of DataFrame and not a column\n",
    "\n",
    "# Assuming the JSON data has keys 'userId', 'productId', and 'interactionValue'\n",
    "interactions_expanded['user_id'] = interactions_expanded['userId']\n",
    "interactions_expanded['product_id'] = interactions_expanded['productId']\n",
    "interactions_expanded['interaction_value'] = interactions_expanded['interactionValue']\n",
    "\n",
    "# Encode user_id and product_id\n",
    "user_ids = interactions_expanded['user_id'].unique().tolist()\n",
    "product_ids = interactions_expanded['product_id'].unique().tolist()\n",
    "\n",
    "user2user_encoded = {x: i for i, x in enumerate(user_ids)}\n",
    "product2product_encoded = {x: i for i, x in enumerate(product_ids)}\n",
    "userencoded2user = {i: x for i, x in enumerate(user_ids)}\n",
    "productencoded2product = {i: x for i, x in enumerate(product_ids)}\n",
    "\n",
    "interactions_expanded['user'] = interactions_expanded['user_id'].map(user2user_encoded)\n",
    "interactions_expanded['product'] = interactions_expanded['product_id'].map(product2product_encoded)\n",
    "\n",
    "# Split the data\n",
    "train, test = train_test_split(interactions_expanded, test_size=0.2, random_state=42)\n",
    "\n",
    "# Convert data into required format\n",
    "x_train = [train['user'].values, train['product'].values]\n",
    "y_train = train['interaction_value'].values\n",
    "x_test = [test['user'].values, test['product'].values]\n",
    "y_test = test['interaction_value'].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_input = Input(shape=(1,))\n",
    "user_embedding = Embedding(len(user2user_encoded), 50)(user_input)  # Removed input_length\n",
    "user_vec = Flatten()(user_embedding)\n",
    "\n",
    "product_input = Input(shape=(1,))\n",
    "product_embedding = Embedding(len(product2product_encoded), 50)(product_input)  # Removed input_length\n",
    "product_vec = Flatten()(product_embedding)\n",
    "\n",
    "dot_product = Dot(axes=1)([user_vec, product_vec])\n",
    "model = Model(inputs=[user_input, product_input], outputs=dot_product)\n",
    "model.compile(optimizer=Adam(), loss='mean_squared_error')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2s/step - loss: 1.0022 - val_loss: 10.0033\n",
      "Epoch 2/20\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 73ms/step - loss: 0.9973 - val_loss: 10.0033\n",
      "Epoch 3/20\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 77ms/step - loss: 0.9924 - val_loss: 10.0033\n",
      "Epoch 4/20\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 77ms/step - loss: 0.9875 - val_loss: 10.0033\n",
      "Epoch 5/20\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 79ms/step - loss: 0.9826 - val_loss: 10.0033\n",
      "Epoch 6/20\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 76ms/step - loss: 0.9777 - val_loss: 10.0033\n",
      "Epoch 7/20\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 75ms/step - loss: 0.9728 - val_loss: 10.0033\n",
      "Epoch 8/20\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 76ms/step - loss: 0.9678 - val_loss: 10.0033\n",
      "Epoch 9/20\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 78ms/step - loss: 0.9629 - val_loss: 10.0033\n",
      "Epoch 10/20\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 74ms/step - loss: 0.9579 - val_loss: 10.0033\n",
      "Epoch 11/20\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 77ms/step - loss: 0.9528 - val_loss: 10.0033\n",
      "Epoch 12/20\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 80ms/step - loss: 0.9477 - val_loss: 10.0033\n",
      "Epoch 13/20\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 77ms/step - loss: 0.9426 - val_loss: 10.0033\n",
      "Epoch 14/20\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 77ms/step - loss: 0.9373 - val_loss: 10.0033\n",
      "Epoch 15/20\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 79ms/step - loss: 0.9320 - val_loss: 10.0033\n",
      "Epoch 16/20\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 75ms/step - loss: 0.9267 - val_loss: 10.0033\n",
      "Epoch 17/20\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 76ms/step - loss: 0.9212 - val_loss: 10.0033\n",
      "Epoch 18/20\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 78ms/step - loss: 0.9156 - val_loss: 10.0033\n",
      "Epoch 19/20\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 76ms/step - loss: 0.9100 - val_loss: 10.0033\n",
      "Epoch 20/20\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 76ms/step - loss: 0.9042 - val_loss: 10.0033\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "The filepath provided must end in `.keras` (Keras model format). Received: filepath=best_model.h5",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 20\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcallbacks\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m EarlyStopping, ModelCheckpoint\n\u001b[0;32m     17\u001b[0m \u001b[38;5;66;03m# Setup callbacks\u001b[39;00m\n\u001b[0;32m     18\u001b[0m callbacks \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m     19\u001b[0m     EarlyStopping(monitor\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mval_loss\u001b[39m\u001b[38;5;124m'\u001b[39m, patience\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, restore_best_weights\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m),\n\u001b[1;32m---> 20\u001b[0m     ModelCheckpoint(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbest_model.h5\u001b[39m\u001b[38;5;124m'\u001b[39m, monitor\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mval_loss\u001b[39m\u001b[38;5;124m'\u001b[39m, save_best_only\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m     21\u001b[0m ]\n\u001b[0;32m     23\u001b[0m \u001b[38;5;66;03m# Fit the model with callbacks\u001b[39;00m\n\u001b[0;32m     24\u001b[0m history \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mfit(\n\u001b[0;32m     25\u001b[0m     x_train, \n\u001b[0;32m     26\u001b[0m     y_train, \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     30\u001b[0m     callbacks\u001b[38;5;241m=\u001b[39mcallbacks\n\u001b[0;32m     31\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\User\\miniconda3\\Lib\\site-packages\\keras\\src\\callbacks\\model_checkpoint.py:191\u001b[0m, in \u001b[0;36mModelCheckpoint.__init__\u001b[1;34m(self, filepath, monitor, verbose, save_best_only, save_weights_only, mode, save_freq, initial_value_threshold)\u001b[0m\n\u001b[0;32m    189\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    190\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfilepath\u001b[38;5;241m.\u001b[39mendswith(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.keras\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m--> 191\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    192\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe filepath provided must end in `.keras` \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    193\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m(Keras model format). Received: \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    194\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfilepath=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfilepath\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    195\u001b[0m         )\n",
      "\u001b[1;31mValueError\u001b[0m: The filepath provided must end in `.keras` (Keras model format). Received: filepath=best_model.h5"
     ]
    }
   ],
   "source": [
    "# Assuming 'train' and 'test' datasets are already split and preprocessed\n",
    "x_train = [np.array(train['user']), np.array(train['product'])]\n",
    "y_train = np.array(train['interaction_value'])\n",
    "\n",
    "# Fit the model\n",
    "history = model.fit(\n",
    "    x_train, \n",
    "    y_train, \n",
    "    epochs=20, \n",
    "    verbose=1, \n",
    "    validation_split=0.2\n",
    ")\n",
    "\n",
    "# Optionally, you can add callbacks, for example to save the best model or early stopping\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "\n",
    "# Setup callbacks\n",
    "callbacks = [\n",
    "    EarlyStopping(monitor='val_loss', patience=5, verbose=1, restore_best_weights=True),\n",
    "    ModelCheckpoint('best_model.h5', monitor='val_loss', save_best_only=True, verbose=1)\n",
    "]\n",
    "\n",
    "# Fit the model with callbacks\n",
    "history = model.fit(\n",
    "    x_train, \n",
    "    y_train, \n",
    "    epochs=20, \n",
    "    verbose=1, \n",
    "    validation_split=0.2,\n",
    "    callbacks=callbacks\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Try Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step\n",
      "Recommended products for user 666545a42b9108ea2b463d87:\n",
      "                        _id    name         category description  price  \\\n",
      "0  6665935c2b9108ea2b463dc2  bababa  asdadaasdadaasd  1234567890     25   \n",
      "\n",
      "                   sellerId  \\\n",
      "0  666545a42b9108ea2b463d87   \n",
      "\n",
      "                                        productImage  __v  \n",
      "0  [https://storage.googleapis.com/kelas-app-test...    0  \n"
     ]
    }
   ],
   "source": [
    "# Function to get recommendations for a specific user\n",
    "def recommend_products(user_id, model, interactions, user2user_encoded, product2product_encoded, productencoded2product, products, top_n=30):\n",
    "    # Check if user_id is in the encoding map\n",
    "    if user_id not in user2user_encoded:\n",
    "        print(f\"User ID {user_id} not found.\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    user_encoded = user2user_encoded[user_id]\n",
    "    # Get all encoded product IDs as a list of integers\n",
    "    product_ids = list(product2product_encoded.values())\n",
    "\n",
    "    # Create user-product array for prediction\n",
    "    # Ensure all entries are integers for the model input\n",
    "    user_product_array = np.array([[user_encoded] * len(product_ids), product_ids]).T.astype(int)\n",
    "\n",
    "    # Predict interaction values using the model\n",
    "    predictions = model.predict([user_product_array[:, 0], user_product_array[:, 1]])\n",
    "    predictions = predictions.flatten()\n",
    "\n",
    "    # Get top N product indices\n",
    "    top_indices = predictions.argsort()[-top_n:][::-1]\n",
    "    # Decode the top indices to product IDs\n",
    "    recommended_product_ids = [productencoded2product[x] for x in top_indices]\n",
    "\n",
    "    # Filter the products DataFrame to get recommended products using the correct column name\n",
    "    recommended_products = products[products['_id'].isin(recommended_product_ids)]\n",
    "    return recommended_products\n",
    "\n",
    "# Try the model with the specified user ID\n",
    "user_id = '666545a42b9108ea2b463d87'\n",
    "recommended_products = recommend_products(user_id, model, interactions, user2user_encoded, product2product_encoded, productencoded2product, products)\n",
    "print(f\"Recommended products for user {user_id}:\")\n",
    "print(recommended_products)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
