{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyP22Q4jqSsjbuGMOkzNwPbo"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["import pandas as pd\n","import tensorflow_hub as hub\n","import numpy as np\n","import tensorflow as tf\n","from sklearn.metrics.pairwise import cosine_similarity"],"metadata":{"id":"MFtLuUve2DcB","executionInfo":{"status":"ok","timestamp":1717397132820,"user_tz":-420,"elapsed":4965,"user":{"displayName":"Viola Amelia M004D4KX1447","userId":"09939378244892352220"}}},"execution_count":4,"outputs":[]},{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"pnmIp2oV1i_M","executionInfo":{"status":"ok","timestamp":1717397063152,"user_tz":-420,"elapsed":25434,"user":{"displayName":"Viola Amelia M004D4KX1447","userId":"09939378244892352220"}},"outputId":"2d803295-7ba1-428c-bc37-f253b9659640"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","source":["file_path = '/content/drive/My Drive/shopee.csv'"],"metadata":{"id":"cPRLnv5X19gf","executionInfo":{"status":"ok","timestamp":1717397329338,"user_tz":-420,"elapsed":322,"user":{"displayName":"Viola Amelia M004D4KX1447","userId":"09939378244892352220"}}},"execution_count":8,"outputs":[]},{"cell_type":"code","source":["data = pd.read_csv(file_path, sep=';')\n","data.head()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":310},"id":"27OtMjVx19c-","executionInfo":{"status":"ok","timestamp":1717401018131,"user_tz":-420,"elapsed":4,"user":{"displayName":"Viola Amelia M004D4KX1447","userId":"09939378244892352220"}},"outputId":"8fe12fb7-e230-4ccf-8a75-1eb11ad61c84"},"execution_count":41,"outputs":[{"output_type":"execute_result","data":{"text/plain":["  Label  Product ID                                              Title Harga  \\\n","0  Meja       11001  Damaindah Meja Belajar Kayu Set Kursi / Meja B...   155   \n","1  Meja       11002  Homedoki Meja / Meja Makan / Meja Komputer / M...   124   \n","2  Meja       11003  Sakula Meja kantor meja kerja Meja Komputer Pe...   107   \n","3  Meja       11004  Meja Portable Stand Laptop Meja Laptop Standin...  99.5   \n","4  Meja       11005  PiPi Furniture Meja Gaming / Meja komputer / M...   446   \n","\n","     Asal Kota  \n","0    Tangerang  \n","1    Tangerang  \n","2  Kab. Gresik  \n","3     Surabaya  \n","4     Surabaya  "],"text/html":["\n","  <div id=\"df-14145f56-a106-4751-9fc3-ff5bc8f4de7b\" class=\"colab-df-container\">\n","    <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Label</th>\n","      <th>Product ID</th>\n","      <th>Title</th>\n","      <th>Harga</th>\n","      <th>Asal Kota</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>Meja</td>\n","      <td>11001</td>\n","      <td>Damaindah Meja Belajar Kayu Set Kursi / Meja B...</td>\n","      <td>155</td>\n","      <td>Tangerang</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>Meja</td>\n","      <td>11002</td>\n","      <td>Homedoki Meja / Meja Makan / Meja Komputer / M...</td>\n","      <td>124</td>\n","      <td>Tangerang</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>Meja</td>\n","      <td>11003</td>\n","      <td>Sakula Meja kantor meja kerja Meja Komputer Pe...</td>\n","      <td>107</td>\n","      <td>Kab. Gresik</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>Meja</td>\n","      <td>11004</td>\n","      <td>Meja Portable Stand Laptop Meja Laptop Standin...</td>\n","      <td>99.5</td>\n","      <td>Surabaya</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>Meja</td>\n","      <td>11005</td>\n","      <td>PiPi Furniture Meja Gaming / Meja komputer / M...</td>\n","      <td>446</td>\n","      <td>Surabaya</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","    <div class=\"colab-df-buttons\">\n","\n","  <div class=\"colab-df-container\">\n","    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-14145f56-a106-4751-9fc3-ff5bc8f4de7b')\"\n","            title=\"Convert this dataframe to an interactive table.\"\n","            style=\"display:none;\">\n","\n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n","    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n","  </svg>\n","    </button>\n","\n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    .colab-df-buttons div {\n","      margin-bottom: 4px;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","    <script>\n","      const buttonEl =\n","        document.querySelector('#df-14145f56-a106-4751-9fc3-ff5bc8f4de7b button.colab-df-convert');\n","      buttonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","      async function convertToInteractive(key) {\n","        const element = document.querySelector('#df-14145f56-a106-4751-9fc3-ff5bc8f4de7b');\n","        const dataTable =\n","          await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                    [key], {});\n","        if (!dataTable) return;\n","\n","        const docLinkHtml = 'Like what you see? Visit the ' +\n","          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","          + ' to learn more about interactive tables.';\n","        element.innerHTML = '';\n","        dataTable['output_type'] = 'display_data';\n","        await google.colab.output.renderOutput(dataTable, element);\n","        const docLink = document.createElement('div');\n","        docLink.innerHTML = docLinkHtml;\n","        element.appendChild(docLink);\n","      }\n","    </script>\n","  </div>\n","\n","\n","<div id=\"df-0f7cb890-529a-4293-ad79-2e3f53f37c93\">\n","  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-0f7cb890-529a-4293-ad79-2e3f53f37c93')\"\n","            title=\"Suggest charts\"\n","            style=\"display:none;\">\n","\n","<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","     width=\"24px\">\n","    <g>\n","        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n","    </g>\n","</svg>\n","  </button>\n","\n","<style>\n","  .colab-df-quickchart {\n","      --bg-color: #E8F0FE;\n","      --fill-color: #1967D2;\n","      --hover-bg-color: #E2EBFA;\n","      --hover-fill-color: #174EA6;\n","      --disabled-fill-color: #AAA;\n","      --disabled-bg-color: #DDD;\n","  }\n","\n","  [theme=dark] .colab-df-quickchart {\n","      --bg-color: #3B4455;\n","      --fill-color: #D2E3FC;\n","      --hover-bg-color: #434B5C;\n","      --hover-fill-color: #FFFFFF;\n","      --disabled-bg-color: #3B4455;\n","      --disabled-fill-color: #666;\n","  }\n","\n","  .colab-df-quickchart {\n","    background-color: var(--bg-color);\n","    border: none;\n","    border-radius: 50%;\n","    cursor: pointer;\n","    display: none;\n","    fill: var(--fill-color);\n","    height: 32px;\n","    padding: 0;\n","    width: 32px;\n","  }\n","\n","  .colab-df-quickchart:hover {\n","    background-color: var(--hover-bg-color);\n","    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n","    fill: var(--button-hover-fill-color);\n","  }\n","\n","  .colab-df-quickchart-complete:disabled,\n","  .colab-df-quickchart-complete:disabled:hover {\n","    background-color: var(--disabled-bg-color);\n","    fill: var(--disabled-fill-color);\n","    box-shadow: none;\n","  }\n","\n","  .colab-df-spinner {\n","    border: 2px solid var(--fill-color);\n","    border-color: transparent;\n","    border-bottom-color: var(--fill-color);\n","    animation:\n","      spin 1s steps(1) infinite;\n","  }\n","\n","  @keyframes spin {\n","    0% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","      border-left-color: var(--fill-color);\n","    }\n","    20% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    30% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","      border-right-color: var(--fill-color);\n","    }\n","    40% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    60% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","    }\n","    80% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-bottom-color: var(--fill-color);\n","    }\n","    90% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","    }\n","  }\n","</style>\n","\n","  <script>\n","    async function quickchart(key) {\n","      const quickchartButtonEl =\n","        document.querySelector('#' + key + ' button');\n","      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n","      quickchartButtonEl.classList.add('colab-df-spinner');\n","      try {\n","        const charts = await google.colab.kernel.invokeFunction(\n","            'suggestCharts', [key], {});\n","      } catch (error) {\n","        console.error('Error during call to suggestCharts:', error);\n","      }\n","      quickchartButtonEl.classList.remove('colab-df-spinner');\n","      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n","    }\n","    (() => {\n","      let quickchartButtonEl =\n","        document.querySelector('#df-0f7cb890-529a-4293-ad79-2e3f53f37c93 button');\n","      quickchartButtonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","    })();\n","  </script>\n","</div>\n","    </div>\n","  </div>\n"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"dataframe","variable_name":"data","summary":"{\n  \"name\": \"data\",\n  \"rows\": 1352,\n  \"fields\": [\n    {\n      \"column\": \"Label\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 6,\n        \"samples\": [\n          \"Meja\",\n          \"Kasur\",\n          \"Setrika\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Product ID\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 390,\n        \"min\": 11001,\n        \"max\": 12352,\n        \"num_unique_values\": 1352,\n        \"samples\": [\n          11050,\n          11639,\n          12033\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Title\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 1287,\n        \"samples\": [\n          \"Setrika Murah Miyako padang\",\n          \"kasur busa density 26 muat 1 orang UK 180x80x10 anti kempes berkualitas\",\n          \"MAGIC COM- RICE COOKER POLYTRON PRC-1202 PRC 1202 GREEN PINK 2LITER\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Harga\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 923,\n        \"samples\": [\n          \"50.5\",\n          \"128000\",\n          \"50\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Asal Kota\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 78,\n        \"samples\": [\n          \"Kab. Bogor\",\n          \"Tangerang\",\n          \"Kab. Semarang\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"}},"metadata":{},"execution_count":41}]},{"cell_type":"code","source":["import numpy as np\n","import pandas as pd\n","import tensorflow as tf\n","from tensorflow.keras.preprocessing.text import Tokenizer\n","from tensorflow.keras.preprocessing.sequence import pad_sequences\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import Embedding, LSTM,RepeatVector, TimeDistributed, Dense\n"],"metadata":{"id":"ZxKDxdIJ6p3V","executionInfo":{"status":"ok","timestamp":1717402527425,"user_tz":-420,"elapsed":337,"user":{"displayName":"Viola Amelia M004D4KX1447","userId":"09939378244892352220"}}},"execution_count":67,"outputs":[]},{"cell_type":"code","source":["texts = data['Title'].astype(str).tolist()  # Ensure the column is converted to strings\n","\n","max_sequence_length = 51  # Maximum length of sequences\n","embedding_dim = 1352  # Embedding dimension for word vectors\n","lstm_units = 64  # Number of units in the LSTM layers\n","\n","# Tokenize the text data\n","vocabulary=tokenizer.word_index\n","tokenizer = Tokenizer(num_words=len(vocabulary) + 1)  # +1 for padding\n","tokenizer.fit_on_texts(texts)\n","encoded_sequences = tokenizer.texts_to_sequences(texts)\n","max_sequence_length = max(len(seq) for seq in sequences)\n","padded_sequences = pad_sequences(encoded_sequences, maxlen=max_sequence_length)\n","\n","# Prepare input-output pairs for training\n","input_texts = np.array([seq[:51] for seq in padded_sequences])\n","output_texts = np.array([seq[1:] for seq in padded_sequences])\n","output_texts = np.array([item for sublist in output_texts for item in sublist])\n","output_texts = np.expand_dims(output_texts, axis=-1)\n","# Debug: Print shapes of input and output data\n","print(f'Input texts shape: {input_texts.shape}')\n","print(f'Output texts shape: {output_texts.shape}')\n","\n","# Define the model\n","model = tf.keras.Sequential([\n","  Embedding(len(vocabulary) + 1, embedding_dim, input_length=max_sequence_length),\n","  LSTM(lstm_units, return_sequences=True),  # Encoder - return sequences for attention\n","  # Insert a Dense layer to convert LSTM output to word indices\n","  Dense(len(vocabulary) + 1, activation='softmax'),\n","  LSTM(lstm_units),\n","  Dense(len(vocabulary) + 1, activation='softmax')\n","])\n","\n","# Compile the model\n","model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n","\n","# Print the model summary\n","model.summary()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"sdjAJXU_19Nf","executionInfo":{"status":"ok","timestamp":1717404696639,"user_tz":-420,"elapsed":1536,"user":{"displayName":"Viola Amelia M004D4KX1447","userId":"09939378244892352220"}},"outputId":"e57f9f81-89df-4e2a-c4bd-80be4884b702"},"execution_count":123,"outputs":[{"output_type":"stream","name":"stdout","text":["Input texts shape: (1352, 51)\n","Output texts shape: (67600, 1)\n","Model: \"sequential_32\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," embedding_34 (Embedding)    (None, 51, 1352)          3120416   \n","                                                                 \n"," lstm_47 (LSTM)              (None, 51, 64)            362752    \n","                                                                 \n"," dense_58 (Dense)            (None, 51, 2308)          150020    \n","                                                                 \n"," lstm_48 (LSTM)              (None, 64)                607488    \n","                                                                 \n"," dense_59 (Dense)            (None, 2308)              150020    \n","                                                                 \n","=================================================================\n","Total params: 4390696 (16.75 MB)\n","Trainable params: 4390696 (16.75 MB)\n","Non-trainable params: 0 (0.00 Byte)\n","_________________________________________________________________\n"]}]},{"cell_type":"code","source":["history = model.fit(input_texts, output_texts, epochs=2, batch_size=32, validation_split=0.2)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"8Eny6wh5188A","executionInfo":{"status":"ok","timestamp":1717404748570,"user_tz":-420,"elapsed":44945,"user":{"displayName":"Viola Amelia M004D4KX1447","userId":"09939378244892352220"}},"outputId":"3f48cea0-e8cc-430c-d396-c24196f681c3"},"execution_count":124,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/2\n","34/34 [==============================] - 22s 539ms/step - loss: 6.6797 - accuracy: 0.6392 - val_loss: 4.3139 - val_accuracy: 0.6568\n","Epoch 2/2\n","34/34 [==============================] - 17s 503ms/step - loss: 2.6308 - accuracy: 0.6605 - val_loss: 2.0417 - val_accuracy: 0.6568\n"]}]},{"cell_type":"code","source":["def predict_autocomplete(model, tokenizer, input_text, max_sequence_length):\n","    input_sequence = tokenizer.texts_to_sequences([input_text])\n","    padded_input = pad_sequences(input_sequence, maxlen=max_sequence_length )\n","    predictions = model.predict(padded_input)\n","    predicted_indices = np.argmax(predictions, axis=-1)\n","    predicted_words = [tokenizer.index_word.get(index, '') for index in predicted_indices]\n","    return predicted_words\n","\n","# Example prediction\n","input_text = \"kasur\"\n","predicted_words = predict_autocomplete(model, tokenizer, input_text, max_sequence_length)\n","print(f\"Predicted words: {predicted_words}\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"zmxUqTlO5-In","executionInfo":{"status":"ok","timestamp":1717404766993,"user_tz":-420,"elapsed":938,"user":{"displayName":"Viola Amelia M004D4KX1447","userId":"09939378244892352220"}},"outputId":"06afe6c1-618f-4b5a-ec40-2f8d5383c580"},"execution_count":126,"outputs":[{"output_type":"stream","name":"stdout","text":["1/1 [==============================] - 1s 819ms/step\n","Predicted words: ['']\n"]}]},{"cell_type":"code","source":["#!/usr/bin/env python\n","\n","\"\"\"\n","Parse all files and write to a single file\n","\"\"\"\n","import os\n","from pathlib import Path\n","from typing import List, NamedTuple\n","\n","from labml import logger, monit\n","\n","from parser import tokenizer\n","from parser.tokenizer import encode, parse_string\n","\n","COMMENT = '#'\n","MULTI_COMMENT = '\"\"\"'\n","\n","\n","class _PythonFile(NamedTuple):\n","    relative_path: str\n","    project: str\n","    path: Path\n","\n","\n","class _GetPythonFiles:\n","    \"\"\"\n","    Get list of python files and their paths inside `data/source` folder\n","    \"\"\"\n","\n","    def __init__(self):\n","        self.source_path = Path(os.getcwd()) / 'data' / 'source'\n","        self.files: List[_PythonFile] = []\n","        self.get_python_files(self.source_path)\n","\n","        logger.inspect([f.path for f in self.files])\n","\n","    def add_file(self, path: Path):\n","        \"\"\"\n","        Add a file to the list of tiles\n","        \"\"\"\n","        project = path.relative_to(self.source_path).parents\n","        project = project[len(project) - 2]\n","        relative_path = path.relative_to(self.source_path / project)\n","\n","        self.files.append(_PythonFile(relative_path=str(relative_path),\n","                                      project=str(project),\n","                                      path=path))\n","\n","    def get_python_files(self, path: Path):\n","        \"\"\"\n","        Recursively collect files\n","        \"\"\"\n","        for p in path.iterdir():\n","            if p.is_dir():\n","                self.get_python_files(p)\n","            else:\n","                if p.suffix == '.py':\n","                    self.add_file(p)\n","\n","\n","def _fix_indentation(parsed: List[tokenizer.ParsedToken]) -> List[tokenizer.ParsedToken]:\n","    \"\"\"\n","    Change indentation tokens. Remove `DEDENT` tokens and\n","    add `INDENT` tokens to each line.\n","    This is easier for prediction.\n","    \"\"\"\n","    res: List[tokenizer.ParsedToken] = []\n","    indentation = 0\n","    indented = False\n","    for t in parsed:\n","        if t.type == tokenizer.TokenType.indent:\n","            indentation += 1\n","        elif t.type == tokenizer.TokenType.dedent:\n","            indentation -= 1\n","        elif t.type in [tokenizer.TokenType.new_line,\n","                        tokenizer.TokenType.eof]:\n","            indented = False\n","            res.append(t)\n","        else:\n","            if not indented:\n","                for _ in range(indentation):\n","                    res.append(tokenizer.ParsedToken(tokenizer.TokenType.indent, 0))\n","                indented = True\n","\n","            res.append(t)\n","\n","    return res\n","\n","\n","def _remove_comments(parsed: List[tokenizer.ParsedToken]) -> List[tokenizer.ParsedToken]:\n","    \"\"\"\n","    Remove comment tokens\n","    \"\"\"\n","    res = []\n","    for p in parsed:\n","        if p.type == tokenizer.TokenType.comment:\n","            continue\n","        else:\n","            res.append(p)\n","\n","    return res\n","\n","\n","def _remove_empty_lines(parsed: List[tokenizer.ParsedToken]) -> List[tokenizer.ParsedToken]:\n","    \"\"\"\n","    Remove empty lines\n","    \"\"\"\n","\n","    tokens = [tokenizer.TokenType.new_line, tokenizer.TokenType.new_line]\n","    res = []\n","    for p in parsed:\n","        for i in range(1):\n","            tokens[i] = tokens[i + 1]\n","        tokens[-1] = p.type\n","        all_new_line = True\n","        for t in tokens:\n","            if t != tokenizer.TokenType.new_line:\n","                all_new_line = False\n","\n","        if all_new_line:\n","            continue\n","        else:\n","            res.append(p)\n","\n","    return res\n","\n","\n","def _read_file(path: Path) -> List[int]:\n","    \"\"\"\n","    Read and encode a file\n","    \"\"\"\n","    with open(str(path)) as f:\n","        content = f.read()\n","\n","    parsed = parse_string(content)\n","    parsed = _remove_comments(parsed)\n","    parsed = _remove_empty_lines(parsed)\n","    parsed = _fix_indentation(parsed)\n","    serialized = encode(parsed)\n","\n","    # deserialized = tokenizer.deserialize(serialized)\n","    # for i in range(len(serialized)):\n","    #     assert deserialized[i] == parsed[i]\n","    #\n","    # res = to_text(deserialized)\n","    # print(res)\n","\n","    return serialized\n","\n","\n","def main():\n","    source_files = _GetPythonFiles().files\n","\n","    logger.inspect(source_files)\n","\n","    with open(str(Path(os.getcwd()) / 'data' / 'all.py'), 'w') as f:\n","        for i, source in monit.enum(\"Parse\", source_files):\n","            serialized = _read_file(source.path)\n","            # return\n","            serialized = [str(t) for t in serialized]\n","            f.write(f\"{str(source.path)}\\n\")\n","            f.write(\" \".join(serialized) + \"\\n\")\n","\n","\n","if __name__ == '__main__':\n","    main()"],"metadata":{"id":"MNwdibd68Zcr"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import math\n","import time\n","import tokenize\n","from io import BytesIO\n","from typing import NamedTuple, List, Tuple\n","\n","import torch\n","import torch.nn\n","from labml import experiment, monit, logger\n","from labml.logger import Text, Style\n","\n","import parser.load\n","import parser.tokenizer\n","from model import SimpleLstmModel\n","from parser import tokenizer\n","\n","# Experiment configuration to load checkpoints\n","experiment.create(name=\"simple_lstm\",\n","                  comment=\"Simple LSTM\")\n","\n","# device to evaluate on\n","device = torch.device(\"cuda:0\")\n","\n","# Beam search\n","BEAM_SIZE = 8\n","\n","\n","class Suggestions(NamedTuple):\n","    codes: List[List[int]]\n","    matched: List[int]\n","    scores: List[float]\n","\n","\n","class ScoredItem(NamedTuple):\n","    score: float\n","    idx: Tuple\n","\n","\n","class Predictor:\n","    \"\"\"\n","    Predicts the next few characters\n","    \"\"\"\n","\n","    NEW_LINE_TOKENS = {tokenize.NEWLINE, tokenize.NL}\n","    INDENT_TOKENS = {tokenize.INDENT, tokenize.DEDENT}\n","\n","    def __init__(self, model, lstm_layers, lstm_size):\n","        self.__model = model\n","\n","        # Initial state\n","        self._h0 = torch.zeros((lstm_layers, 1, lstm_size), device=device)\n","        self._c0 = torch.zeros((lstm_layers, 1, lstm_size), device=device)\n","\n","        # Last line of source code read\n","        self._last_line = \"\"\n","\n","        self._tokens: List[tokenize.TokenInfo] = []\n","\n","        # Last token, because we need to input that to the model for inference\n","        self._last_token = 0\n","\n","        # Last bit of the input string\n","        self._untokenized = \"\"\n","\n","        # For timing\n","        self.time_add = 0\n","        self.time_predict = 0\n","        self.time_check = 0\n","\n","    def __clear_tokens(self, lines: int):\n","        \"\"\"\n","        Clears old lines from tokens\n","        \"\"\"\n","        for i, t in enumerate(self._tokens):\n","            if t.type in self.NEW_LINE_TOKENS:\n","                lines -= 1\n","                if lines == 0:\n","                    self._tokens = self._tokens[i + 1:]\n","                    return\n","\n","        raise RuntimeError()\n","\n","    def __clear_untokenized(self, tokens):\n","        \"\"\"\n","        Remove tokens not properly tokenized;\n","         i.e. the last token, unless it's a new line\n","        \"\"\"\n","\n","        limit = 0\n","        for i in reversed(range(len(tokens))):\n","            if tokens[i].type in self.NEW_LINE_TOKENS:\n","                limit = i + 1\n","                break\n","            else:\n","                limit = i\n","                break\n","\n","        return tokens[:limit]\n","\n","    @staticmethod\n","    def __get_tokens(it):\n","        tokens: List[tokenize.TokenInfo] = []\n","\n","        try:\n","            for t in it:\n","                if t.type in tokenizer.SKIP_TOKENS:\n","                    continue\n","                if t.type == tokenize.NEWLINE and t.string == '':\n","                    continue\n","                if t.type == tokenize.DEDENT:\n","                    continue\n","                if t.type == tokenize.ERRORTOKEN:\n","                    continue\n","                tokens.append(t)\n","        except tokenize.TokenError as e:\n","            if not e.args[0].startswith('EOF in'):\n","                print(e)\n","        except IndentationError as e:\n","            print(e)\n","\n","        return tokens\n","\n","    def add(self, content):\n","        \"\"\"\n","        Add a string of code, this shouldn't have multiple lines\n","        \"\"\"\n","        start_time = time.time()\n","        self._last_line += content\n","\n","        # Remove old lines\n","        lines = self._last_line.split(\"\\n\")\n","        if len(lines) > 1:\n","            assert len(lines) <= 3\n","            if lines[-1] == '':\n","                if len(lines) > 2:\n","                    self.__clear_tokens(len(lines) - 2)\n","                    lines = lines[-2:]\n","            else:\n","                self.__clear_tokens(len(lines) - 1)\n","                lines = lines[-1:]\n","\n","        line = '\\n'.join(lines)\n","\n","        self._last_line = line\n","\n","        # Parse the last line\n","        tokens_it = tokenize.tokenize(BytesIO(self._last_line.encode('utf-8')).readline)\n","        tokens = self.__get_tokens(tokens_it)\n","\n","        # Remove last token\n","        tokens = self.__clear_untokenized(tokens)\n","\n","        # Check if previous tokens is a prefix\n","        assert len(tokens) >= len(self._tokens)\n","\n","        for t1, t2 in zip(self._tokens, tokens):\n","            assert t1.type == t2.type\n","            assert t1.string == t2.string\n","\n","        # Get the untokenized string\n","        if len(tokens) > 0:\n","            assert tokens[-1].end[0] == 1\n","            self._untokenized = line[tokens[-1].end[1]:]\n","        else:\n","            self._untokenized = line\n","\n","        # Update previous tokens and the model state\n","        if len(tokens) > len(self._tokens):\n","            self.__update_state(tokens[len(self._tokens):])\n","            self._tokens = tokens\n","\n","        self.time_add += time.time() - start_time\n","\n","    def get_predictions(self, codes_batch: List[List[int]]):\n","        # Sequence length and batch size\n","        seq_len = len(codes_batch[0])\n","        batch_size = len(codes_batch)\n","\n","        for codes in codes_batch:\n","            assert seq_len == len(codes)\n","\n","        # Input to the model\n","        x = torch.tensor(codes_batch, device=device)\n","        x = x.transpose(0, 1)\n","\n","        # Expand state\n","        h0 = self._h0.expand(-1, batch_size, -1).contiguous()\n","        c0 = self._c0.expand(-1, batch_size, -1).contiguous()\n","\n","        # Get predictions\n","        prediction, _, _ = self.__model(x, h0, c0)\n","\n","        assert prediction.shape == (seq_len, len(codes_batch), tokenizer.VOCAB_SIZE)\n","\n","        # Final prediction\n","        prediction = prediction[-1, :, :]\n","\n","        return prediction.detach().cpu().numpy()\n","\n","    def get_suggestion(self) -> str:\n","        # Start of with the last token\n","        suggestions = [Suggestions([[self._last_token]],\n","                                   [0],\n","                                   [1.])]\n","\n","        # Do a beam search, up to the untokenized string length and 10 more\n","        for step in range(10 + len(self._untokenized)):\n","            sugg = suggestions[step]\n","            batch_size = len(sugg.codes)\n","\n","            # Break if empty\n","            if batch_size == 0:\n","                break\n","\n","            # Get predictions\n","            start_time = time.time()\n","            predictions = self.get_predictions(sugg.codes)\n","            self.time_predict += time.time() - start_time\n","\n","            start_time = time.time()\n","            # Get all choices\n","            choices = []\n","            for idx in range(batch_size):\n","                for code in range(tokenizer.VOCAB_SIZE):\n","                    score = sugg.scores[idx] * predictions[idx, code]\n","                    choices.append(ScoredItem(\n","                        score * math.sqrt(sugg.matched[idx] + tokenizer.LENGTHS[code]),\n","                        (idx, code)))\n","            # Sort them\n","            choices.sort(key=lambda x: x.score, reverse=True)\n","\n","            # Collect the ones that match untokenized string\n","            codes = []\n","            matches = []\n","            scores = []\n","            len_untokenized = len(self._untokenized)\n","\n","            for choice in choices:\n","                prev_idx = choice.idx[0]\n","                code = choice.idx[1]\n","\n","                token = tokenizer.DESERIALIZE[code]\n","                if token.type in tokenizer.LINE_BREAK:\n","                    continue\n","\n","                # Previously mached length\n","                matched = sugg.matched[prev_idx]\n","\n","                if matched >= len_untokenized:\n","                    # Increment the length if already matched\n","                    matched += tokenizer.LENGTHS[code]\n","                else:\n","                    # Otherwise check if the new token string matches\n","                    unmatched = tokenizer.DECODE[code][sugg.codes[prev_idx][-1]]\n","                    to_match = self._untokenized[matched:]\n","\n","                    if len(unmatched) < len(to_match):\n","                        if not to_match.startswith(unmatched):\n","                            continue\n","                        else:\n","                            matched += len(unmatched)\n","                    else:\n","                        if not unmatched.startswith(to_match):\n","                            continue\n","                        else:\n","                            matched += len(unmatched)\n","\n","                # Collect new item\n","                codes.append(sugg.codes[prev_idx] + [code])\n","                matches.append(matched)\n","                score = sugg.scores[prev_idx] * predictions[prev_idx, code]\n","                scores.append(score)\n","\n","                # Stop at `BEAM_SIZE`\n","                if len(scores) == BEAM_SIZE:\n","                    break\n","\n","            suggestions.append(Suggestions(codes, matches, scores))\n","\n","            self.time_check += time.time() - start_time\n","\n","        # Collect suggestions of all lengths\n","        choices = []\n","        for s_idx, sugg in enumerate(suggestions):\n","            batch_size = len(sugg.codes)\n","            for idx in range(batch_size):\n","                length = sugg.matched[idx] - len(self._untokenized)\n","                if length <= 2:\n","                    continue\n","                choice = sugg.scores[idx] * math.sqrt(length - 1)\n","                choices.append(ScoredItem(choice, (s_idx, idx)))\n","        choices.sort(key=lambda x: x.score, reverse=True)\n","\n","        # Return the best option\n","        for choice in choices:\n","            codes = suggestions[choice.idx[0]].codes[choice.idx[1]]\n","            res = \"\"\n","            prev = self._last_token\n","            for code in codes[1:]:\n","                res += tokenizer.DECODE[code][prev]\n","                prev = code\n","\n","            res = res[len(self._untokenized):]\n","\n","            # Skip if blank\n","            if res.strip() == \"\":\n","                continue\n","\n","            return res\n","\n","        # Return blank if there are no options\n","        return ''\n","\n","    def __update_state(self, tokens):\n","        \"\"\"\n","        Update model state\n","        \"\"\"\n","        data = parser.tokenizer.parse(tokens)\n","        data = parser.tokenizer.encode(data)\n","        x = [self._last_token] + data[:-1]\n","        self._last_token = data[-1]\n","\n","        x = torch.tensor([x], device=device)\n","        x = x.transpose(0, 1)\n","        _, _, (hn, cn) = self.__model(x, self._h0, self._c0)\n","        self._h0 = hn.detach()\n","        self._c0 = cn.detach()\n","\n","\n","class Evaluator:\n","    def __init__(self, model, file: parser.load.EncodedFile,\n","                 lstm_layers, lstm_size,\n","                 skip_spaces=False):\n","        self.__content = self.get_content(file.codes)\n","        self.__skip_spaces = skip_spaces\n","        self.__predictor = Predictor(model, lstm_layers, lstm_size)\n","\n","    @staticmethod\n","    def get_content(codes: List[int]):\n","        tokens = parser.tokenizer.decode(codes)\n","        content = parser.tokenizer.to_string(tokens)\n","        return content.split('\\n')\n","\n","    def eval(self):\n","        keys_saved = 0\n","\n","        for line, content in enumerate(self.__content):\n","            # Keep reference to rest of the line\n","            rest_of_line = content\n","\n","            # Build the line for logging with colors\n","            # The line number\n","            logs = [(f\"{line: 4d}: \", Text.meta)]\n","\n","            # Type the line character by character\n","            while rest_of_line != '':\n","                suggestion = self.__predictor.get_suggestion()\n","\n","                # If suggestion matches\n","                if suggestion != '' and rest_of_line.startswith(suggestion):\n","                    # Log\n","                    logs.append((suggestion[0], [Style.underline, Text.danger]))\n","                    logs.append((suggestion[1:], Style.underline))\n","\n","                    keys_saved += len(suggestion) - 1\n","\n","                    # Skip the prediction text\n","                    rest_of_line = rest_of_line[len(suggestion):]\n","\n","                    # Add text to the predictor\n","                    self.__predictor.add(suggestion)\n","\n","                # If the suggestion doesn't match\n","                else:\n","                    # Add the next character\n","                    self.__predictor.add(rest_of_line[0])\n","                    logs.append((rest_of_line[0], Text.subtle))\n","                    rest_of_line = rest_of_line[1:]\n","\n","            # Add a new line\n","            self.__predictor.add(\"\\n\")\n","\n","            # Log the line\n","            logger.log(logs)\n","\n","        # Log time taken for the file\n","        logger.inspect(add=self.__predictor.time_add,\n","                       check=self.__predictor.time_check,\n","                       predict=self.__predictor.time_predict)\n","\n","        total_keys = sum([len(c) for c in self.__content])\n","        logger.inspect(keys_saved=keys_saved,\n","                       percentage_saved=100 * keys_saved / total_keys,\n","                       total_keys=total_keys,\n","                       total_lines=len(self.__content))\n","\n","\n","def main():\n","    lstm_size = 1024\n","    lstm_layers = 3\n","\n","    with monit.section(\"Loading data\"):\n","        files = parser.load.load_files()\n","        train_files, valid_files = parser.load.split_train_valid(files, is_shuffle=False)\n","\n","    with monit.section(\"Create model\"):\n","        model = SimpleLstmModel(encoding_size=tokenizer.VOCAB_SIZE,\n","                                embedding_size=tokenizer.VOCAB_SIZE,\n","                                lstm_size=lstm_size,\n","                                lstm_layers=lstm_layers)\n","        model.to(device)\n","\n","    experiment.add_pytorch_models({'base': model})\n","\n","    experiment.load(\"2a86d636936d11eab8740dffb016e7b1\", 72237)\n","\n","    # For debugging with a specific piece of source code\n","    # predictor = Predictor(model, lstm_layers, lstm_size)\n","    # for s in ['\"\"\" \"\"\"\\n', \"from __future__\"]:\n","    #     predictor.add(s)\n","    # s = predictor.get_suggestion()\n","\n","    # Evaluate all the files in validation set\n","    for file in valid_files:\n","        logger.log(str(file.path), Text.heading)\n","        evaluator = Evaluator(model, file,\n","                              lstm_layers, lstm_size,\n","                              skip_spaces=True)\n","        evaluator.eval()\n","\n","\n","if __name__ == '__main__':\n","    main()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":397},"id":"iIgnsKCh8ZM6","executionInfo":{"status":"error","timestamp":1717399074415,"user_tz":-420,"elapsed":1969,"user":{"displayName":"Viola Amelia M004D4KX1447","userId":"09939378244892352220"}},"outputId":"60384966-40b4-41e3-8821-a8879c4b3f49"},"execution_count":35,"outputs":[{"output_type":"error","ename":"ModuleNotFoundError","evalue":"No module named 'parser'","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)","\u001b[0;32m<ipython-input-35-ac7d5d3a76c9>\u001b[0m in \u001b[0;36m<cell line: 12>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mlabml\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogger\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mText\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mStyle\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mparser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mparser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSimpleLstmModel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'parser'","","\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"],"errorDetails":{"actions":[{"action":"open_url","actionText":"Open Examples","url":"/notebooks/snippets/importing_libraries.ipynb"}]}}]},{"cell_type":"code","source":["!pip install parser\n"],"metadata":{"id":"ofitGYIx9bsq"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import math\n","from typing import List\n","\n","import numpy as np\n","import torch\n","import torch.nn\n","from labml import experiment, monit, tracker, logger\n","from labml.utils.delayed_keyboard_interrupt import DelayedKeyboardInterrupt\n","\n","import parser.load\n","from model import SimpleLstmModel\n","from parser import tokenizer\n","\n","# Setup the experiment\n","experiment.create(name=\"simple_lstm\",\n","                  comment=\"Simple LSTM\")\n","\n","# device to train on\n","device = torch.device(\"cuda:0\")\n","\n","\n","def list_to_batches(x, batch_size, batches, seq_len):\n","    \"\"\"\n","    Prepare flat data into batches to be ready for the model to consume\n","    \"\"\"\n","    x = np.reshape(x, (batch_size, batches, seq_len))\n","    x = np.transpose(x, (1, 2, 0))\n","\n","    return x\n","\n","\n","def get_batches(files: List[parser.load.EncodedFile], eof: int, batch_size=32, seq_len=32):\n","    \"\"\"\n","    Covert raw encoded files into trainin/validation batches\n","    \"\"\"\n","\n","    # Shuffle the order of files\n","    np.random.shuffle(files)\n","\n","    # Concatenate all the files whilst adding `eof` marker at the beginnings\n","    data = []\n","    for f in files:\n","        data.append(eof)\n","        data += f.codes\n","    data = np.array(data)\n","\n","    # Start from a random offset\n","    offset = np.random.randint(seq_len * batch_size)\n","    data = data[offset:]\n","\n","    # Number of batches\n","    batches = (len(data) - 1) // batch_size // seq_len\n","\n","    # Extract input\n","    x = data[:(batch_size * seq_len * batches)]\n","    # Extract output, i.e. the next char\n","    y = data[1:(batch_size * seq_len * batches) + 1]\n","\n","    # Covert the flat data into batches\n","    x = list_to_batches(x, batch_size, batches, seq_len)\n","    y = list_to_batches(y, batch_size, batches, seq_len)\n","\n","    return x, y\n","\n","\n","class Trainer:\n","    \"\"\"\n","    This will maintain states, data and train/validate the model\n","    \"\"\"\n","\n","    def __init__(self, *, files: List[parser.load.EncodedFile],\n","                 model, loss_func, optimizer,\n","                 eof: int,\n","                 batch_size: int, seq_len: int,\n","                 is_train: bool,\n","                 h0, c0):\n","        # Get batches\n","        x, y = get_batches(files, eof,\n","                           batch_size=batch_size,\n","                           seq_len=seq_len)\n","        # Covert data to PyTorch tensors\n","        self.x = torch.tensor(x, device=device)\n","        self.y = torch.tensor(y, device=device)\n","\n","        # Initial state\n","        self.hn = h0\n","        self.cn = c0\n","\n","        self.model = model\n","        self.loss_func = loss_func\n","        self.optimizer = optimizer\n","        self.p = None\n","        self.is_train = is_train\n","\n","    def run(self, i):\n","        # Get model output\n","        self.p, logits, (self.hn, self.cn) = self.model(self.x[i], self.hn, self.cn)\n","\n","        # Flatten outputs\n","        logits = logits.view(-1, self.p.shape[-1])\n","        yi = self.y[i].reshape(-1)\n","\n","        # Calculate loss\n","        loss = self.loss_func(logits, yi)\n","\n","        # Store the states\n","        self.hn = self.hn.detach()\n","        self.cn = self.cn.detach()\n","\n","        if self.is_train:\n","            # Take a training step\n","            self.optimizer.zero_grad()\n","            loss.backward()\n","            self.optimizer.step()\n","\n","            tracker.add(\"train.loss\", loss.cpu().data.item())\n","        else:\n","            tracker.add(\"valid.loss\", loss.cpu().data.item())\n","\n","\n","def main_train():\n","    lstm_size = 1024\n","    lstm_layers = 3\n","    batch_size = 32\n","    seq_len = 32\n","\n","    with monit.section(\"Loading data\"):\n","        # Load all python files\n","        files = parser.load.load_files()\n","        # Split training and validation data\n","        train_files, valid_files = parser.load.split_train_valid(files, is_shuffle=False)\n","\n","    with monit.section(\"Create model\"):\n","        # Create model\n","        model = SimpleLstmModel(encoding_size=tokenizer.VOCAB_SIZE,\n","                                embedding_size=tokenizer.VOCAB_SIZE,\n","                                lstm_size=lstm_size,\n","                                lstm_layers=lstm_layers)\n","        # Move model to `device`\n","        model.to(device)\n","\n","        # Create loss function and optimizer\n","        loss_func = torch.nn.CrossEntropyLoss()\n","        optimizer = torch.optim.Adam(model.parameters())\n","\n","    # Initial state is 0\n","    h0 = torch.zeros((lstm_layers, batch_size, lstm_size), device=device)\n","    c0 = torch.zeros((lstm_layers, batch_size, lstm_size), device=device)\n","\n","    # Setup logger indicators\n","    tracker.set_queue(\"train.loss\", queue_size=500, is_print=True)\n","    tracker.set_queue(\"valid.loss\", queue_size=500, is_print=True)\n","\n","    # Specify the model in [lab](https://github.com/vpj/lab) for saving and loading\n","    experiment.add_pytorch_models({'base': model})\n","\n","    # Start training scratch (step '0')\n","    experiment.start()\n","\n","    # Number of batches per epoch\n","    batches = math.ceil(sum([len(f[1]) + 1 for f in train_files]) / (batch_size * seq_len))\n","\n","    # Number of steps per epoch. We train and validate on each step.\n","    steps_per_epoch = 200\n","\n","    # Train for 100 epochs\n","    for epoch in monit.loop(range(100)):\n","        # Create trainer\n","        trainer = Trainer(files=train_files,\n","                          model=model,\n","                          loss_func=loss_func,\n","                          optimizer=optimizer,\n","                          batch_size=batch_size,\n","                          seq_len=seq_len,\n","                          is_train=True,\n","                          h0=h0,\n","                          c0=c0,\n","                          eof=0)\n","        # Create validator\n","        validator = Trainer(files=valid_files,\n","                            model=model,\n","                            loss_func=loss_func,\n","                            optimizer=optimizer,\n","                            is_train=False,\n","                            seq_len=seq_len,\n","                            batch_size=batch_size,\n","                            h0=h0,\n","                            c0=c0,\n","                            eof=0)\n","\n","        # Next batch to train and validation\n","        train_batch = 0\n","        valid_batch = 0\n","\n","        # Loop through steps\n","        for i in range(1, steps_per_epoch):\n","            try:\n","                with DelayedKeyboardInterrupt():\n","                    # Set global step\n","                    global_step = epoch * batches + min(batches, (batches * i) // steps_per_epoch)\n","                    tracker.set_global_step(global_step)\n","\n","                    # Last batch to train and validate\n","                    train_batch_limit = trainer.x.shape[0] * min(1., (i + 1) / steps_per_epoch)\n","                    valid_batch_limit = validator.x.shape[0] * min(1., (i + 1) / steps_per_epoch)\n","\n","                    with monit.section(\"train\", total_steps=trainer.x.shape[0], is_partial=True):\n","                        model.train()\n","                        # Train\n","                        while train_batch < train_batch_limit:\n","                            trainer.run(train_batch)\n","                            monit.progress(train_batch + 1)\n","                            train_batch += 1\n","\n","                    with monit.section(\"valid\", total_steps=validator.x.shape[0], is_partial=True):\n","                        model.eval()\n","                        # Validate\n","                        while valid_batch < valid_batch_limit:\n","                            validator.run(valid_batch)\n","                            monit.progress(valid_batch + 1)\n","                            valid_batch += 1\n","\n","                    # Output results\n","                    tracker.save()\n","\n","                    # 10 lines of logs per epoch\n","                    if (i + 1) % (steps_per_epoch // 10) == 0:\n","                        logger.log()\n","            except KeyboardInterrupt:\n","                experiment.save_checkpoint()\n","                return\n","\n","        experiment.save_checkpoint()\n","\n","\n","if __name__ == '__main__':\n","    main_train()"],"metadata":{"id":"Oz9VdRQI9ThH"},"execution_count":null,"outputs":[]}]}